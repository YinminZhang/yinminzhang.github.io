<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">

<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js">/*pace*/</script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
<style>
    .pace .pace-progress {
        background: #1E92FB; /*进度条颜色*/
        height: 3px;
    }
    .pace .pace-progress-inner {
         box-shadow: 0 0 10px #1E92FB, 0 0 5px     #1E92FB; /*阴影颜色*/
    }
    .pace .pace-activity {
        border-top-color: #1E92FB;    /*上边框颜色*/
        border-left-color: #1E92FB;    /*左边框颜色*/
    }
</style>

<meta name="theme-color" content="#222">






















<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">

<link rel="stylesheet" href="/css/main.css?v=7.1.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.2">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.2" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="人脸生成（Face Generation）在该项目中，你将使用生成式对抗网络（Generative Adversarial Nets）来生成新的人脸图像。 获取数据该项目将使用以下数据集：  MNIST CelebA  由于 CelebA 数据集比较复杂，而且这是你第一次使用 GANs。我们想让你先在 MNIST 数据集上测试你的 GANs 模型，以让你更快的评估所建立模型的性能。 如果你在使用">
<meta name="keywords" content="python">
<meta property="og:type" content="article">
<meta property="og:title" content="Face Generation">
<meta property="og:url" content="https://yinminzhang.github.io/blog/2019/06/27/1/index.html">
<meta property="og:site_name" content="Yinmin Zhang&#39; Blog">
<meta property="og:description" content="人脸生成（Face Generation）在该项目中，你将使用生成式对抗网络（Generative Adversarial Nets）来生成新的人脸图像。 获取数据该项目将使用以下数据集：  MNIST CelebA  由于 CelebA 数据集比较复杂，而且这是你第一次使用 GANs。我们想让你先在 MNIST 数据集上测试你的 GANs 模型，以让你更快的评估所建立模型的性能。 如果你在使用">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_3_1.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_5_1.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_23_1.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_23_3.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_23_5.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_23_7.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_23_9.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_23_11.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_23_13.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_23_15.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_23_17.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_23_19.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_23_21.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_23_23.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_23_25.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_23_27.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_23_29.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_23_31.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_23_33.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_23_35.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_1.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_3.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_5.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_7.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_9.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_11.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_13.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_15.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_17.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_19.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_21.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_23.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_25.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_27.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_29.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_31.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_33.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_35.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_37.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_39.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_41.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_43.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_45.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_47.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_49.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_51.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_53.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_55.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_57.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_59.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_61.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_63.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_65.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_67.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_69.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_71.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_73.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_75.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_77.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_79.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_81.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_83.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_85.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_87.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_89.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_91.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_93.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_95.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_97.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_99.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_101.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_103.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_105.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_107.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_109.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_111.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_113.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_115.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_117.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_119.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_121.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_123.png">
<meta property="og:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_25_125.png">
<meta property="og:updated_time" content="2019-06-26T13:09:01.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Face Generation">
<meta name="twitter:description" content="人脸生成（Face Generation）在该项目中，你将使用生成式对抗网络（Generative Adversarial Nets）来生成新的人脸图像。 获取数据该项目将使用以下数据集：  MNIST CelebA  由于 CelebA 数据集比较复杂，而且这是你第一次使用 GANs。我们想让你先在 MNIST 数据集上测试你的 GANs 模型，以让你更快的评估所建立模型的性能。 如果你在使用">
<meta name="twitter:image" content="https://yinminzhang.github.io/blog/2019/06/27/1/output_3_1.png">



  <link rel="alternate" href="/atom.xml" title="Yinmin Zhang' Blog" type="application/atom+xml">



  
  
  <link rel="canonical" href="https://yinminzhang.github.io/blog/2019/06/27/1/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Face Generation | Yinmin Zhang' Blog</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Yinmin Zhang' Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>时间轴</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    
  
  

  

  <a href="https://github.com/YisraelZhang" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a>



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://yinminzhang.github.io/blog/2019/06/27/1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yinmin Zhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yinmin Zhang' Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Face Generation

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-27 14:08:56" itemprop="dateCreated datePublished" datetime="2019-06-27T14:08:56+08:00">2019-06-27</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-26 21:09:01" itemprop="dateModified" datetime="2019-06-26T21:09:01+08:00">2019-06-26</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Deep-learning/" itemprop="url" rel="index"><span itemprop="name">Deep learning</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/27/1/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/06/27/1/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/2019/06/27/1/" class="leancloud_visitors" data-flag-title="Face Generation">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">热度：</span>
              
                <span class="leancloud-visitors-count"></span>
                <span>℃</span>
            </span>
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">本文字数：</span>
                
                <span title="本文字数">79k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">1:12</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="人脸生成（Face-Generation）"><a href="#人脸生成（Face-Generation）" class="headerlink" title="人脸生成（Face Generation）"></a>人脸生成（Face Generation）</h1><p>在该项目中，你将使用生成式对抗网络（Generative Adversarial Nets）来生成新的人脸图像。</p>
<h3 id="获取数据"><a href="#获取数据" class="headerlink" title="获取数据"></a>获取数据</h3><p>该项目将使用以下数据集：</p>
<ul>
<li>MNIST</li>
<li>CelebA</li>
</ul>
<p>由于 CelebA 数据集比较复杂，而且这是你第一次使用 GANs。我们想让你先在 MNIST 数据集上测试你的 GANs 模型，以让你更快的评估所建立模型的性能。</p>
<p>如果你在使用 <a href="https://www.floydhub.com/" target="_blank" rel="noopener">FloydHub</a>, 请将 <code>data_dir</code> 设置为 “/input” 并使用 <a href="http://docs.floydhub.com/home/using_datasets/" target="_blank" rel="noopener">FloydHub data ID</a> “R5KrjnANiKVhLWAkpXhNBe”.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">data_dir = <span class="string">'./data'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># FloydHub - Use with data ID "R5KrjnANiKVhLWAkpXhNBe"</span></span><br><span class="line"><span class="comment">#data_dir = '/input'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">DON'T MODIFY ANYTHING IN THIS CELL</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> helper</span><br><span class="line"></span><br><span class="line">helper.download_extract(<span class="string">'mnist'</span>, data_dir)</span><br><span class="line">helper.download_extract(<span class="string">'celeba'</span>, data_dir)</span><br></pre></td></tr></table></figure>

<pre><code>Found mnist Data
Found celeba Data</code></pre><h2 id="探索数据（Explore-the-Data）"><a href="#探索数据（Explore-the-Data）" class="headerlink" title="探索数据（Explore the Data）"></a>探索数据（Explore the Data）</h2><h3 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a>MNIST</h3><p><a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">MNIST</a> 是一个手写数字的图像数据集。你可以更改 <code>show_n_images</code> 探索此数据集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">show_n_images = <span class="number">25</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">DON'T MODIFY ANYTHING IN THIS CELL</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> glob <span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"></span><br><span class="line">mnist_images = helper.get_batch(glob(os.path.join(data_dir, <span class="string">'mnist/*.jpg'</span>))[:show_n_images], <span class="number">28</span>, <span class="number">28</span>, <span class="string">'L'</span>)</span><br><span class="line">pyplot.imshow(helper.images_square_grid(mnist_images, <span class="string">'L'</span>), cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>&lt;matplotlib.image.AxesImage at 0x213247f2358&gt;</code></pre><p><img src="output_3_1.png" alt="png"></p>
<h3 id="CelebA"><a href="#CelebA" class="headerlink" title="CelebA"></a>CelebA</h3><p><a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" target="_blank" rel="noopener">CelebFaces Attributes Dataset (CelebA)</a> 是一个包含 20 多万张名人图片及相关图片说明的数据集。你将用此数据集生成人脸，不会用不到相关说明。你可以更改 <code>show_n_images</code> 探索此数据集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">show_n_images = <span class="number">25</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">DON'T MODIFY ANYTHING IN THIS CELL</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">mnist_images = helper.get_batch(glob(os.path.join(data_dir, <span class="string">'img_align_celeba/*.jpg'</span>))[:show_n_images], <span class="number">28</span>, <span class="number">28</span>, <span class="string">'RGB'</span>)</span><br><span class="line">pyplot.imshow(helper.images_square_grid(mnist_images, <span class="string">'RGB'</span>))</span><br></pre></td></tr></table></figure>

<pre><code>&lt;matplotlib.image.AxesImage at 0x2132486a9b0&gt;</code></pre><p><img src="output_5_1.png" alt="png"></p>
<h2 id="预处理数据（Preprocess-the-Data）"><a href="#预处理数据（Preprocess-the-Data）" class="headerlink" title="预处理数据（Preprocess the Data）"></a>预处理数据（Preprocess the Data）</h2><p>由于该项目的重点是建立 GANs 模型，我们将为你预处理数据。</p>
<p>经过数据预处理，MNIST 和 CelebA 数据集的值在 28×28 维度图像的 [-0.5, 0.5] 范围内。CelebA 数据集中的图像裁剪了非脸部的图像部分，然后调整到 28x28 维度。</p>
<p>MNIST 数据集中的图像是单<a href="https://en.wikipedia.org/wiki/Channel_(digital_image%29" target="_blank" rel="noopener">通道</a>的黑白图像，CelebA 数据集中的图像是 <a href="https://en.wikipedia.org/wiki/Channel_(digital_image%29#RGB_Images" target="_blank" rel="noopener">三通道的 RGB 彩色图像</a>。</p>
<h2 id="建立神经网络（Build-the-Neural-Network）"><a href="#建立神经网络（Build-the-Neural-Network）" class="headerlink" title="建立神经网络（Build the Neural Network）"></a>建立神经网络（Build the Neural Network）</h2><p>你将通过部署以下函数来建立 GANs 的主要组成部分:</p>
<ul>
<li><code>model_inputs</code></li>
<li><code>discriminator</code></li>
<li><code>generator</code></li>
<li><code>model_loss</code></li>
<li><code>model_opt</code></li>
<li><code>train</code></li>
</ul>
<h3 id="检查-TensorFlow-版本并获取-GPU-型号"><a href="#检查-TensorFlow-版本并获取-GPU-型号" class="headerlink" title="检查 TensorFlow 版本并获取 GPU 型号"></a>检查 TensorFlow 版本并获取 GPU 型号</h3><p>检查你是否使用正确的 TensorFlow 版本，并获取 GPU 型号</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">DON'T MODIFY ANYTHING IN THIS CELL</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">from</span> distutils.version <span class="keyword">import</span> LooseVersion</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check TensorFlow Version</span></span><br><span class="line"><span class="keyword">assert</span> LooseVersion(tf.__version__) &gt;= LooseVersion(<span class="string">'1.0'</span>), <span class="string">'Please use TensorFlow version 1.0 or newer.  You are using &#123;&#125;'</span>.format(tf.__version__)</span><br><span class="line">print(<span class="string">'TensorFlow Version: &#123;&#125;'</span>.format(tf.__version__))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check for a GPU</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> tf.test.gpu_device_name():</span><br><span class="line">    warnings.warn(<span class="string">'No GPU found. Please use a GPU to train your neural network.'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'Default GPU Device: &#123;&#125;'</span>.format(tf.test.gpu_device_name()))</span><br></pre></td></tr></table></figure>

<pre><code>D:\Users\Lenovo\Anaconda3\envs\tensorflow-gpu\lib\importlib\_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)


TensorFlow Version: 1.9.0
Default GPU Device: /device:GPU:0</code></pre><h3 id="输入（Input）"><a href="#输入（Input）" class="headerlink" title="输入（Input）"></a>输入（Input）</h3><p>部署 <code>model_inputs</code> 函数以创建用于神经网络的 <a href="https://www.tensorflow.org/versions/r0.11/api_docs/python/io_ops/placeholders" target="_blank" rel="noopener">占位符 (TF Placeholders)</a>。请创建以下占位符：</p>
<ul>
<li>输入图像占位符: 使用 <code>image_width</code>，<code>image_height</code> 和 <code>image_channels</code> 设置为 rank 4。</li>
<li>输入 Z 占位符: 设置为 rank 2，并命名为 <code>z_dim</code>。</li>
<li>学习速率占位符: 设置为 rank 0。</li>
</ul>
<p>返回占位符元组的形状为 (tensor of real input images, tensor of z data, learning rate)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> problem_unittests <span class="keyword">as</span> tests</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_inputs</span><span class="params">(image_width, image_height, image_channels, z_dim)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Create the model inputs</span></span><br><span class="line"><span class="string">    :param image_width: The input image width</span></span><br><span class="line"><span class="string">    :param image_height: The input image height</span></span><br><span class="line"><span class="string">    :param image_channels: The number of image channels</span></span><br><span class="line"><span class="string">    :param z_dim: The dimension of Z</span></span><br><span class="line"><span class="string">    :return: Tuple of (tensor of real input images, tensor of z data, learning rate)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement Function</span></span><br><span class="line">    image_input = tf.placeholder(tf.float32, [<span class="literal">None</span>, image_width, image_height, image_channels], name=<span class="string">'input_real'</span>)</span><br><span class="line">    z_input = tf.placeholder(tf.float32, [<span class="literal">None</span>, z_dim], name=<span class="string">'input_z'</span>)</span><br><span class="line">    learning_rate = tf.placeholder(tf.float32, [], name=<span class="string">'learning_rate'</span>)</span><br><span class="line">    <span class="keyword">return</span> image_input, z_input, learning_rate</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">tests.test_model_inputs(model_inputs)</span><br></pre></td></tr></table></figure>

<pre><code>Tests Passed</code></pre><h3 id="辨别器（Discriminator）"><a href="#辨别器（Discriminator）" class="headerlink" title="辨别器（Discriminator）"></a>辨别器（Discriminator）</h3><p>部署 <code>discriminator</code> 函数创建辨别器神经网络以辨别 <code>images</code>。该函数应能够重复使用神经网络中的各种变量。 在 <a href="https://www.tensorflow.org/api_docs/python/tf/variable_scope" target="_blank" rel="noopener"><code>tf.variable_scope</code></a> 中使用 “discriminator” 的变量空间名来重复使用该函数中的变量。 </p>
<p>该函数应返回形如 (tensor output of the discriminator, tensor logits of the discriminator) 的元组。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">discriminator</span><span class="params">(images, reuse=False, alpha=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Create the discriminator network</span></span><br><span class="line"><span class="string">    :param image: Tensor of input image(s)</span></span><br><span class="line"><span class="string">    :param reuse: Boolean if the weights should be reused</span></span><br><span class="line"><span class="string">    :return: Tuple of (tensor output of the discriminator, tensor logits of the discriminator)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement Function</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"discriminator"</span>, reuse=reuse):</span><br><span class="line">        conv1 = tf.layers.conv2d(images,<span class="number">64</span>, <span class="number">5</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>, activation=<span class="literal">None</span>, kernel_initializer= tf.contrib.layers.xavier_initializer())</span><br><span class="line">        relu1 = tf.maximum(alpha*conv1, conv1)</span><br><span class="line"></span><br><span class="line">        conv2 = tf.layers.conv2d(relu1, <span class="number">128</span>, <span class="number">5</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>, activation=<span class="literal">None</span>, kernel_initializer= tf.contrib.layers.xavier_initializer())</span><br><span class="line">        bn2 = tf.layers.batch_normalization(conv2, trainable=<span class="literal">True</span>)</span><br><span class="line">        relu2 = tf.maximum(alpha*bn2, bn2)</span><br><span class="line"></span><br><span class="line">        conv3 = tf.layers.conv2d(relu2, <span class="number">256</span>, <span class="number">5</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>, activation=<span class="literal">None</span>, kernel_initializer= tf.contrib.layers.xavier_initializer())</span><br><span class="line">        bn3 = tf.layers.batch_normalization(conv3, trainable=<span class="literal">True</span>)</span><br><span class="line">        relu3 = tf.maximum(alpha*bn3, bn3)</span><br><span class="line"></span><br><span class="line">        flatten = tf.layers.flatten(relu3)</span><br><span class="line">        logits = tf.layers.dense(flatten, <span class="number">1</span>)</span><br><span class="line">        out = tf.nn.sigmoid(logits)</span><br><span class="line">    <span class="keyword">return</span> out, logits</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">tests.test_discriminator(discriminator, tf)</span><br></pre></td></tr></table></figure>

<pre><code>Tests Passed</code></pre><h3 id="生成器（Generator）"><a href="#生成器（Generator）" class="headerlink" title="生成器（Generator）"></a>生成器（Generator）</h3><p>部署 <code>generator</code> 函数以使用 <code>z</code> 生成图像。该函数应能够重复使用神经网络中的各种变量。<br>在 <a href="https://www.tensorflow.org/api_docs/python/tf/variable_scope" target="_blank" rel="noopener"><code>tf.variable_scope</code></a> 中使用 “generator” 的变量空间名来重复使用该函数中的变量。 </p>
<p>该函数应返回所生成的 28 x 28 x <code>out_channel_dim</code> 维度图像。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">(z, out_channel_dim, is_train=True, alpha=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Create the generator network</span></span><br><span class="line"><span class="string">    :param z: Input z</span></span><br><span class="line"><span class="string">    :param out_channel_dim: The number of channels in the output image</span></span><br><span class="line"><span class="string">    :param is_train: Boolean if generator is being used for training</span></span><br><span class="line"><span class="string">    :return: The tensor output of the generator</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement Function</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'generator'</span>, reuse=<span class="keyword">not</span> is_train):</span><br><span class="line">        x1 = tf.layers.dense(z, <span class="number">7</span>*<span class="number">7</span>*<span class="number">512</span>)</span><br><span class="line">        x1 = tf.reshape(x1, (<span class="number">-1</span>, <span class="number">7</span>, <span class="number">7</span>, <span class="number">512</span>))</span><br><span class="line">        bn1 = tf.layers.batch_normalization(x1, trainable=is_train)</span><br><span class="line">        relu1 = tf.maximum(alpha*bn1, bn1)</span><br><span class="line">        </span><br><span class="line">        x2 = tf.layers.conv2d_transpose(relu1, <span class="number">256</span>, <span class="number">5</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>, activation=<span class="literal">None</span>, kernel_initializer= tf.contrib.layers.xavier_initializer())</span><br><span class="line">        bn2 = tf.layers.batch_normalization(x2, trainable=is_train)</span><br><span class="line">        relu2 = tf.maximum(alpha*bn2, bn2)</span><br><span class="line">        </span><br><span class="line">        logist = tf.layers.conv2d_transpose(relu2, out_channel_dim, <span class="number">5</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>, activation=<span class="literal">None</span>, kernel_initializer= tf.contrib.layers.xavier_initializer())</span><br><span class="line">        out = tf.tanh(logist)</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">tests.test_generator(generator, tf)</span><br></pre></td></tr></table></figure>

<pre><code>Tests Passed</code></pre><h3 id="损失函数（Loss）"><a href="#损失函数（Loss）" class="headerlink" title="损失函数（Loss）"></a>损失函数（Loss）</h3><p>部署 <code>model_loss</code> 函数训练并计算 GANs 的损失。该函数应返回形如 (discriminator loss, generator loss) 的元组。</p>
<p>使用你已实现的函数：</p>
<ul>
<li><code>discriminator(images, reuse=False)</code></li>
<li><code>generator(z, out_channel_dim, is_train=True)</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_loss</span><span class="params">(input_real, input_z, out_channel_dim, smooth=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Get the loss for the discriminator and generator</span></span><br><span class="line"><span class="string">    :param input_real: Images from the real dataset</span></span><br><span class="line"><span class="string">    :param input_z: Z input</span></span><br><span class="line"><span class="string">    :param out_channel_dim: The number of channels in the output image</span></span><br><span class="line"><span class="string">    :return: A tuple of (discriminator loss, generator loss)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement Function</span></span><br><span class="line">    g_model=generator(input_z, out_channel_dim, is_train=<span class="literal">True</span>)</span><br><span class="line">    d_model_real,d_real_logits=discriminator(input_real)</span><br><span class="line">    d_model_fake,d_fake_logits=discriminator(g_model,reuse=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    smooth=<span class="number">0.1</span></span><br><span class="line">    d_loss_real=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_real_logits,labels=tf.ones_like(d_real_logits) * (<span class="number">1</span> - smooth)))</span><br><span class="line">    d_loss_fake=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_fake_logits,labels=tf.zeros_like(d_model_fake)))</span><br><span class="line">    g_loss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_fake_logits,labels=tf.ones_like(d_model_fake)))</span><br><span class="line">    </span><br><span class="line">    d_loss=d_loss_real+d_loss_fake</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> d_loss,g_loss</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">tests.test_model_loss(model_loss)</span><br></pre></td></tr></table></figure>

<pre><code>Tests Passed</code></pre><h3 id="优化（Optimization）"><a href="#优化（Optimization）" class="headerlink" title="优化（Optimization）"></a>优化（Optimization）</h3><p>部署 <code>model_opt</code> 函数实现对 GANs 的优化。使用 <a href="https://www.tensorflow.org/api_docs/python/tf/trainable_variables" target="_blank" rel="noopener"><code>tf.trainable_variables</code></a> 获取可训练的所有变量。通过变量空间名 <code>discriminator</code> 和 <code>generator</code> 来过滤变量。该函数应返回形如 (discriminator training operation, generator training operation) 的元组。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_opt</span><span class="params">(d_loss, g_loss, learning_rate, beta1)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Get optimization operations</span></span><br><span class="line"><span class="string">    :param d_loss: Discriminator loss Tensor</span></span><br><span class="line"><span class="string">    :param g_loss: Generator loss Tensor</span></span><br><span class="line"><span class="string">    :param learning_rate: Learning Rate Placeholder</span></span><br><span class="line"><span class="string">    :param beta1: The exponential decay rate for the 1st moment in the optimizer</span></span><br><span class="line"><span class="string">    :return: A tuple of (discriminator training operation, generator training operation)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement Function</span></span><br><span class="line">    </span><br><span class="line">    t_vars = tf.trainable_variables()</span><br><span class="line">    d_vars = [var <span class="keyword">for</span> var <span class="keyword">in</span> t_vars <span class="keyword">if</span> var.name.startswith(<span class="string">'discriminator'</span>)]</span><br><span class="line">    g_vars = [var <span class="keyword">for</span> var <span class="keyword">in</span> t_vars <span class="keyword">if</span> var.name.startswith(<span class="string">'generator'</span>)]</span><br><span class="line">    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies(update_ops):</span><br><span class="line">        </span><br><span class="line">        d_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list=d_vars)</span><br><span class="line">        g_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(g_loss, var_list=g_vars)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> d_train_opt, g_train_opt</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">tests.test_model_opt(model_opt, tf)</span><br></pre></td></tr></table></figure>

<pre><code>Tests Passed</code></pre><h2 id="训练神经网络（Neural-Network-Training）"><a href="#训练神经网络（Neural-Network-Training）" class="headerlink" title="训练神经网络（Neural Network Training）"></a>训练神经网络（Neural Network Training）</h2><h3 id="输出显示"><a href="#输出显示" class="headerlink" title="输出显示"></a>输出显示</h3><p>使用该函数可以显示生成器 (Generator) 在训练过程中的当前输出，这会帮你评估 GANs 模型的训练程度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">DON'T MODIFY ANYTHING IN THIS CELL</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_generator_output</span><span class="params">(sess, n_images, input_z, out_channel_dim, image_mode)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Show example output for the generator</span></span><br><span class="line"><span class="string">    :param sess: TensorFlow session</span></span><br><span class="line"><span class="string">    :param n_images: Number of Images to display</span></span><br><span class="line"><span class="string">    :param input_z: Input Z Tensor</span></span><br><span class="line"><span class="string">    :param out_channel_dim: The number of channels in the output image</span></span><br><span class="line"><span class="string">    :param image_mode: The mode to use for images ("RGB" or "L")</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    cmap = <span class="literal">None</span> <span class="keyword">if</span> image_mode == <span class="string">'RGB'</span> <span class="keyword">else</span> <span class="string">'gray'</span></span><br><span class="line">    z_dim = input_z.get_shape().as_list()[<span class="number">-1</span>]</span><br><span class="line">    example_z = np.random.uniform(<span class="number">-1</span>, <span class="number">1</span>, size=[n_images, z_dim])</span><br><span class="line"></span><br><span class="line">    samples = sess.run(</span><br><span class="line">        generator(input_z, out_channel_dim, <span class="literal">False</span>),</span><br><span class="line">        feed_dict=&#123;input_z: example_z&#125;)</span><br><span class="line"></span><br><span class="line">    images_grid = helper.images_square_grid(samples, image_mode)</span><br><span class="line">    pyplot.imshow(images_grid, cmap=cmap)</span><br><span class="line">    pyplot.show()</span><br></pre></td></tr></table></figure>

<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>部署 <code>train</code> 函数以建立并训练 GANs 模型。记得使用以下你已完成的函数：</p>
<ul>
<li><code>model_inputs(image_width, image_height, image_channels, z_dim)</code></li>
<li><code>model_loss(input_real, input_z, out_channel_dim)</code></li>
<li><code>model_opt(d_loss, g_loss, learning_rate, beta1)</code></li>
</ul>
<p>使用 <code>show_generator_output</code> 函数显示 <code>generator</code> 在训练过程中的输出。</p>
<p><strong>注意</strong>：在每个批次 (batch) 中运行 <code>show_generator_output</code> 函数会显著增加训练时间与该 notebook 的体积。推荐每 100 批次输出一次 <code>generator</code> 的输出。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(epoch_count, batch_size, z_dim, learning_rate, beta1, get_batches, data_shape, data_image_mode)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Train the GAN</span></span><br><span class="line"><span class="string">    :param epoch_count: Number of epochs</span></span><br><span class="line"><span class="string">    :param batch_size: Batch Size</span></span><br><span class="line"><span class="string">    :param z_dim: Z dimension</span></span><br><span class="line"><span class="string">    :param learning_rate: Learning Rate</span></span><br><span class="line"><span class="string">    :param beta1: The exponential decay rate for the 1st moment in the optimizer</span></span><br><span class="line"><span class="string">    :param get_batches: Function to get batches</span></span><br><span class="line"><span class="string">    :param data_shape: Shape of the data</span></span><br><span class="line"><span class="string">    :param data_image_mode: The image mode to use for images ("RGB" or "L")</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Build Model</span></span><br><span class="line">    input_real, input_z, lr = model_inputs(data_shape[<span class="number">1</span>], data_shape[<span class="number">2</span>], data_shape[<span class="number">3</span>], z_dim)</span><br><span class="line">    d_loss, g_loss = model_loss(input_real, input_z, data_shape[<span class="number">3</span>])</span><br><span class="line">    d_opt, g_opt = model_opt(d_loss, g_loss, lr, beta1)</span><br><span class="line">    </span><br><span class="line">    samples, losses = [], []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        sess.run(tf.global_variables_initializer())</span><br><span class="line">        <span class="keyword">for</span> epoch_i <span class="keyword">in</span> range(epoch_count):</span><br><span class="line">            steps = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> batch_images <span class="keyword">in</span> get_batches(batch_size):</span><br><span class="line">                <span class="comment"># <span class="doctag">TODO:</span> Train Model</span></span><br><span class="line">                steps += <span class="number">1</span></span><br><span class="line">                batch_images = batch_images * <span class="number">2</span></span><br><span class="line">                batch_z = np.random.uniform(<span class="number">-1</span>, <span class="number">1</span>, size=(batch_size, z_dim))</span><br><span class="line">                </span><br><span class="line">                _ = sess.run(d_opt, feed_dict=&#123;input_real:batch_images, input_z:batch_z, lr:learning_rate&#125;)</span><br><span class="line">                _ = sess.run(g_opt, feed_dict=&#123;input_z:batch_z, lr:learning_rate&#125;)</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> steps%<span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">                    train_loss_d = d_loss.eval(&#123;input_real:batch_images, input_z:batch_z&#125;)</span><br><span class="line">                    train_loss_g = g_loss.eval(&#123;input_z:batch_z&#125;)</span><br><span class="line">                    </span><br><span class="line">                    print(<span class="string">"Epoch &#123;&#125;/&#123;&#125;..."</span>.format(epoch_i+<span class="number">1</span>, epochs),</span><br><span class="line">                          <span class="string">"Batch &#123;&#125;..."</span>.format(steps),</span><br><span class="line">                          <span class="string">"Discriminator Loss: &#123;:.4f&#125;..."</span>.format(train_loss_d),</span><br><span class="line">                          <span class="string">"Generator Loss: &#123;:.4f&#125;"</span>.format(train_loss_g))</span><br><span class="line">                <span class="keyword">if</span> steps%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                    show_generator_output(sess, show_n_images, input_z, data_shape[<span class="number">3</span>], data_image_mode)</span><br></pre></td></tr></table></figure>

<h3 id="MNIST-1"><a href="#MNIST-1" class="headerlink" title="MNIST"></a>MNIST</h3><p>在 MNIST 上测试你的 GANs 模型。经过 2 次迭代，GANs 应该能够生成类似手写数字的图像。确保生成器 (generator) 低于辨别器 (discriminator) 的损失，或接近 0。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">z_dim = <span class="number">128</span></span><br><span class="line">learning_rate = <span class="number">0.0002</span></span><br><span class="line">beta1 = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">epochs = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">mnist_dataset = helper.Dataset(<span class="string">'mnist'</span>, glob(os.path.join(data_dir, <span class="string">'mnist/*.jpg'</span>)))</span><br><span class="line"><span class="keyword">with</span> tf.Graph().as_default():</span><br><span class="line">    train(epochs, batch_size, z_dim, learning_rate, beta1, mnist_dataset.get_batches,</span><br><span class="line">          mnist_dataset.shape, mnist_dataset.image_mode)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/2... Batch 10... Discriminator Loss: 0.9627... Generator Loss: 0.7875
Epoch 1/2... Batch 20... Discriminator Loss: 0.9174... Generator Loss: 0.8130
Epoch 1/2... Batch 30... Discriminator Loss: 0.7045... Generator Loss: 1.2363
Epoch 1/2... Batch 40... Discriminator Loss: 0.7240... Generator Loss: 2.1126
Epoch 1/2... Batch 50... Discriminator Loss: 0.5128... Generator Loss: 1.8915
Epoch 1/2... Batch 60... Discriminator Loss: 0.8241... Generator Loss: 1.0503
Epoch 1/2... Batch 70... Discriminator Loss: 0.7746... Generator Loss: 1.4643
Epoch 1/2... Batch 80... Discriminator Loss: 0.5734... Generator Loss: 1.8580
Epoch 1/2... Batch 90... Discriminator Loss: 0.5254... Generator Loss: 2.3466
Epoch 1/2... Batch 100... Discriminator Loss: 0.5721... Generator Loss: 3.0006</code></pre><p><img src="output_23_1.png" alt="png"></p>
<pre><code>Epoch 1/2... Batch 110... Discriminator Loss: 0.4951... Generator Loss: 2.8843
Epoch 1/2... Batch 120... Discriminator Loss: 0.6080... Generator Loss: 1.9425
Epoch 1/2... Batch 130... Discriminator Loss: 0.6788... Generator Loss: 1.9291
Epoch 1/2... Batch 140... Discriminator Loss: 0.5260... Generator Loss: 2.1446
Epoch 1/2... Batch 150... Discriminator Loss: 0.5936... Generator Loss: 2.2602
Epoch 1/2... Batch 160... Discriminator Loss: 0.9277... Generator Loss: 1.1499
Epoch 1/2... Batch 170... Discriminator Loss: 0.7266... Generator Loss: 2.0648
Epoch 1/2... Batch 180... Discriminator Loss: 0.9038... Generator Loss: 1.4034
Epoch 1/2... Batch 190... Discriminator Loss: 0.6906... Generator Loss: 2.4101
Epoch 1/2... Batch 200... Discriminator Loss: 0.6886... Generator Loss: 2.0620</code></pre><p><img src="output_23_3.png" alt="png"></p>
<pre><code>Epoch 1/2... Batch 210... Discriminator Loss: 0.9148... Generator Loss: 1.5641
Epoch 1/2... Batch 220... Discriminator Loss: 1.1484... Generator Loss: 1.0639
Epoch 1/2... Batch 230... Discriminator Loss: 0.9669... Generator Loss: 1.3079
Epoch 1/2... Batch 240... Discriminator Loss: 0.7315... Generator Loss: 1.7730
Epoch 1/2... Batch 250... Discriminator Loss: 0.8537... Generator Loss: 1.7153
Epoch 1/2... Batch 260... Discriminator Loss: 0.8256... Generator Loss: 1.6790
Epoch 1/2... Batch 270... Discriminator Loss: 0.8870... Generator Loss: 2.2118
Epoch 1/2... Batch 280... Discriminator Loss: 0.9213... Generator Loss: 1.4661
Epoch 1/2... Batch 290... Discriminator Loss: 0.7770... Generator Loss: 1.9560
Epoch 1/2... Batch 300... Discriminator Loss: 0.8099... Generator Loss: 2.1415</code></pre><p><img src="output_23_5.png" alt="png"></p>
<pre><code>Epoch 1/2... Batch 310... Discriminator Loss: 0.8594... Generator Loss: 2.2313
Epoch 1/2... Batch 320... Discriminator Loss: 0.9495... Generator Loss: 1.7784
Epoch 1/2... Batch 330... Discriminator Loss: 0.8249... Generator Loss: 1.7010
Epoch 1/2... Batch 340... Discriminator Loss: 0.8699... Generator Loss: 1.9127
Epoch 1/2... Batch 350... Discriminator Loss: 0.7958... Generator Loss: 1.8774
Epoch 1/2... Batch 360... Discriminator Loss: 0.8858... Generator Loss: 1.2416
Epoch 1/2... Batch 370... Discriminator Loss: 0.8622... Generator Loss: 1.4814
Epoch 1/2... Batch 380... Discriminator Loss: 0.8390... Generator Loss: 1.5083
Epoch 1/2... Batch 390... Discriminator Loss: 0.9377... Generator Loss: 1.2274
Epoch 1/2... Batch 400... Discriminator Loss: 0.7196... Generator Loss: 2.0331</code></pre><p><img src="output_23_7.png" alt="png"></p>
<pre><code>Epoch 1/2... Batch 410... Discriminator Loss: 0.8298... Generator Loss: 1.6503
Epoch 1/2... Batch 420... Discriminator Loss: 0.7834... Generator Loss: 1.5748
Epoch 1/2... Batch 430... Discriminator Loss: 0.8089... Generator Loss: 2.0889
Epoch 1/2... Batch 440... Discriminator Loss: 0.8478... Generator Loss: 1.5553
Epoch 1/2... Batch 450... Discriminator Loss: 0.8038... Generator Loss: 1.5093
Epoch 1/2... Batch 460... Discriminator Loss: 0.7762... Generator Loss: 2.0020
Epoch 1/2... Batch 470... Discriminator Loss: 0.9601... Generator Loss: 1.3426
Epoch 1/2... Batch 480... Discriminator Loss: 0.7570... Generator Loss: 2.1164
Epoch 1/2... Batch 490... Discriminator Loss: 0.7255... Generator Loss: 1.8155
Epoch 1/2... Batch 500... Discriminator Loss: 0.7742... Generator Loss: 1.6010</code></pre><p><img src="output_23_9.png" alt="png"></p>
<pre><code>Epoch 1/2... Batch 510... Discriminator Loss: 0.9511... Generator Loss: 1.4050
Epoch 1/2... Batch 520... Discriminator Loss: 0.7391... Generator Loss: 1.8236
Epoch 1/2... Batch 530... Discriminator Loss: 0.8647... Generator Loss: 1.6992
Epoch 1/2... Batch 540... Discriminator Loss: 0.6579... Generator Loss: 1.6406
Epoch 1/2... Batch 550... Discriminator Loss: 0.6725... Generator Loss: 2.1059
Epoch 1/2... Batch 560... Discriminator Loss: 0.9139... Generator Loss: 1.3907
Epoch 1/2... Batch 570... Discriminator Loss: 0.7361... Generator Loss: 2.0791
Epoch 1/2... Batch 580... Discriminator Loss: 0.7166... Generator Loss: 1.9413
Epoch 1/2... Batch 590... Discriminator Loss: 0.8389... Generator Loss: 1.3885
Epoch 1/2... Batch 600... Discriminator Loss: 0.7775... Generator Loss: 2.2436</code></pre><p><img src="output_23_11.png" alt="png"></p>
<pre><code>Epoch 1/2... Batch 610... Discriminator Loss: 0.7631... Generator Loss: 2.0067
Epoch 1/2... Batch 620... Discriminator Loss: 0.8102... Generator Loss: 1.5625
Epoch 1/2... Batch 630... Discriminator Loss: 0.8196... Generator Loss: 1.8408
Epoch 1/2... Batch 640... Discriminator Loss: 0.7188... Generator Loss: 1.9150
Epoch 1/2... Batch 650... Discriminator Loss: 0.9867... Generator Loss: 1.1708
Epoch 1/2... Batch 660... Discriminator Loss: 0.7941... Generator Loss: 1.7650
Epoch 1/2... Batch 670... Discriminator Loss: 0.8267... Generator Loss: 1.9731
Epoch 1/2... Batch 680... Discriminator Loss: 0.9433... Generator Loss: 1.1614
Epoch 1/2... Batch 690... Discriminator Loss: 0.8899... Generator Loss: 1.3595
Epoch 1/2... Batch 700... Discriminator Loss: 0.7991... Generator Loss: 2.4998</code></pre><p><img src="output_23_13.png" alt="png"></p>
<pre><code>Epoch 1/2... Batch 710... Discriminator Loss: 0.9115... Generator Loss: 1.4783
Epoch 1/2... Batch 720... Discriminator Loss: 0.8680... Generator Loss: 1.3549
Epoch 1/2... Batch 730... Discriminator Loss: 0.7776... Generator Loss: 1.8143
Epoch 1/2... Batch 740... Discriminator Loss: 0.8594... Generator Loss: 1.5717
Epoch 1/2... Batch 750... Discriminator Loss: 0.8480... Generator Loss: 1.9891
Epoch 1/2... Batch 760... Discriminator Loss: 0.8691... Generator Loss: 1.7379
Epoch 1/2... Batch 770... Discriminator Loss: 0.9642... Generator Loss: 2.5907
Epoch 1/2... Batch 780... Discriminator Loss: 0.9028... Generator Loss: 1.3877
Epoch 1/2... Batch 790... Discriminator Loss: 1.0270... Generator Loss: 0.9990
Epoch 1/2... Batch 800... Discriminator Loss: 0.8572... Generator Loss: 1.4752</code></pre><p><img src="output_23_15.png" alt="png"></p>
<pre><code>Epoch 1/2... Batch 810... Discriminator Loss: 0.8880... Generator Loss: 1.5381
Epoch 1/2... Batch 820... Discriminator Loss: 0.8298... Generator Loss: 1.4535
Epoch 1/2... Batch 830... Discriminator Loss: 0.8860... Generator Loss: 1.2786
Epoch 1/2... Batch 840... Discriminator Loss: 0.8882... Generator Loss: 2.2311
Epoch 1/2... Batch 850... Discriminator Loss: 0.8000... Generator Loss: 1.6295
Epoch 1/2... Batch 860... Discriminator Loss: 0.8723... Generator Loss: 1.1965
Epoch 1/2... Batch 870... Discriminator Loss: 0.9579... Generator Loss: 1.2390
Epoch 1/2... Batch 880... Discriminator Loss: 0.7378... Generator Loss: 1.8470
Epoch 1/2... Batch 890... Discriminator Loss: 0.9657... Generator Loss: 1.0971
Epoch 1/2... Batch 900... Discriminator Loss: 0.8642... Generator Loss: 1.5896</code></pre><p><img src="output_23_17.png" alt="png"></p>
<pre><code>Epoch 1/2... Batch 910... Discriminator Loss: 1.0032... Generator Loss: 1.3101
Epoch 1/2... Batch 920... Discriminator Loss: 0.9020... Generator Loss: 1.3205
Epoch 1/2... Batch 930... Discriminator Loss: 0.8533... Generator Loss: 1.5136
Epoch 2/2... Batch 10... Discriminator Loss: 0.9416... Generator Loss: 1.2490
Epoch 2/2... Batch 20... Discriminator Loss: 0.9764... Generator Loss: 1.1918
Epoch 2/2... Batch 30... Discriminator Loss: 1.0604... Generator Loss: 1.9076
Epoch 2/2... Batch 40... Discriminator Loss: 0.9644... Generator Loss: 1.0420
Epoch 2/2... Batch 50... Discriminator Loss: 1.0065... Generator Loss: 1.4173
Epoch 2/2... Batch 60... Discriminator Loss: 1.0605... Generator Loss: 0.9198
Epoch 2/2... Batch 70... Discriminator Loss: 0.9821... Generator Loss: 1.0946
Epoch 2/2... Batch 80... Discriminator Loss: 0.8265... Generator Loss: 1.5769
Epoch 2/2... Batch 90... Discriminator Loss: 0.9507... Generator Loss: 2.1309
Epoch 2/2... Batch 100... Discriminator Loss: 0.8979... Generator Loss: 1.2493</code></pre><p><img src="output_23_19.png" alt="png"></p>
<pre><code>Epoch 2/2... Batch 110... Discriminator Loss: 0.9850... Generator Loss: 2.3388
Epoch 2/2... Batch 120... Discriminator Loss: 0.8296... Generator Loss: 2.0036
Epoch 2/2... Batch 130... Discriminator Loss: 0.8423... Generator Loss: 1.6824
Epoch 2/2... Batch 140... Discriminator Loss: 1.0202... Generator Loss: 1.7160
Epoch 2/2... Batch 150... Discriminator Loss: 0.8977... Generator Loss: 1.5909
Epoch 2/2... Batch 160... Discriminator Loss: 0.9121... Generator Loss: 1.5185
Epoch 2/2... Batch 170... Discriminator Loss: 0.8746... Generator Loss: 1.5487
Epoch 2/2... Batch 180... Discriminator Loss: 0.9337... Generator Loss: 1.5802
Epoch 2/2... Batch 190... Discriminator Loss: 0.9722... Generator Loss: 1.7139
Epoch 2/2... Batch 200... Discriminator Loss: 0.8821... Generator Loss: 1.2507</code></pre><p><img src="output_23_21.png" alt="png"></p>
<pre><code>Epoch 2/2... Batch 210... Discriminator Loss: 0.8134... Generator Loss: 1.9155
Epoch 2/2... Batch 220... Discriminator Loss: 0.9798... Generator Loss: 1.2178
Epoch 2/2... Batch 230... Discriminator Loss: 0.9703... Generator Loss: 1.1664
Epoch 2/2... Batch 240... Discriminator Loss: 1.1046... Generator Loss: 1.1118
Epoch 2/2... Batch 250... Discriminator Loss: 0.8787... Generator Loss: 1.4165
Epoch 2/2... Batch 260... Discriminator Loss: 0.8558... Generator Loss: 2.2611
Epoch 2/2... Batch 270... Discriminator Loss: 1.2768... Generator Loss: 2.2397
Epoch 2/2... Batch 280... Discriminator Loss: 0.9799... Generator Loss: 1.3003
Epoch 2/2... Batch 290... Discriminator Loss: 1.1653... Generator Loss: 0.8523
Epoch 2/2... Batch 300... Discriminator Loss: 0.9194... Generator Loss: 1.5541</code></pre><p><img src="output_23_23.png" alt="png"></p>
<pre><code>Epoch 2/2... Batch 310... Discriminator Loss: 0.9504... Generator Loss: 1.2357
Epoch 2/2... Batch 320... Discriminator Loss: 1.0290... Generator Loss: 1.5459
Epoch 2/2... Batch 330... Discriminator Loss: 0.9386... Generator Loss: 1.4610
Epoch 2/2... Batch 340... Discriminator Loss: 1.0624... Generator Loss: 0.9162
Epoch 2/2... Batch 350... Discriminator Loss: 0.8907... Generator Loss: 1.8487
Epoch 2/2... Batch 360... Discriminator Loss: 1.0106... Generator Loss: 1.3382
Epoch 2/2... Batch 370... Discriminator Loss: 0.9335... Generator Loss: 1.3463
Epoch 2/2... Batch 380... Discriminator Loss: 0.9040... Generator Loss: 1.3094
Epoch 2/2... Batch 390... Discriminator Loss: 0.9716... Generator Loss: 1.3650
Epoch 2/2... Batch 400... Discriminator Loss: 1.0267... Generator Loss: 1.0680</code></pre><p><img src="output_23_25.png" alt="png"></p>
<pre><code>Epoch 2/2... Batch 410... Discriminator Loss: 1.0693... Generator Loss: 1.0447
Epoch 2/2... Batch 420... Discriminator Loss: 0.8992... Generator Loss: 1.4296
Epoch 2/2... Batch 430... Discriminator Loss: 1.0428... Generator Loss: 1.3955
Epoch 2/2... Batch 440... Discriminator Loss: 0.9420... Generator Loss: 1.2113
Epoch 2/2... Batch 450... Discriminator Loss: 0.8857... Generator Loss: 1.4255
Epoch 2/2... Batch 460... Discriminator Loss: 0.9020... Generator Loss: 1.3917
Epoch 2/2... Batch 470... Discriminator Loss: 1.0779... Generator Loss: 1.7242
Epoch 2/2... Batch 480... Discriminator Loss: 0.9909... Generator Loss: 1.0841
Epoch 2/2... Batch 490... Discriminator Loss: 1.0229... Generator Loss: 0.9844
Epoch 2/2... Batch 500... Discriminator Loss: 0.9904... Generator Loss: 1.2607</code></pre><p><img src="output_23_27.png" alt="png"></p>
<pre><code>Epoch 2/2... Batch 510... Discriminator Loss: 1.0860... Generator Loss: 1.6045
Epoch 2/2... Batch 520... Discriminator Loss: 0.9960... Generator Loss: 1.3987
Epoch 2/2... Batch 530... Discriminator Loss: 0.9151... Generator Loss: 1.2345
Epoch 2/2... Batch 540... Discriminator Loss: 1.0506... Generator Loss: 0.9963
Epoch 2/2... Batch 550... Discriminator Loss: 0.9258... Generator Loss: 1.4107
Epoch 2/2... Batch 560... Discriminator Loss: 1.1630... Generator Loss: 0.7236
Epoch 2/2... Batch 570... Discriminator Loss: 1.1497... Generator Loss: 0.9518
Epoch 2/2... Batch 580... Discriminator Loss: 0.9784... Generator Loss: 1.2324
Epoch 2/2... Batch 590... Discriminator Loss: 0.8726... Generator Loss: 1.4524
Epoch 2/2... Batch 600... Discriminator Loss: 0.9668... Generator Loss: 1.2295</code></pre><p><img src="output_23_29.png" alt="png"></p>
<pre><code>Epoch 2/2... Batch 610... Discriminator Loss: 1.0421... Generator Loss: 2.0669
Epoch 2/2... Batch 620... Discriminator Loss: 0.9521... Generator Loss: 1.2010
Epoch 2/2... Batch 630... Discriminator Loss: 0.9434... Generator Loss: 1.3111
Epoch 2/2... Batch 640... Discriminator Loss: 0.9762... Generator Loss: 1.0773
Epoch 2/2... Batch 650... Discriminator Loss: 1.1472... Generator Loss: 0.8091
Epoch 2/2... Batch 660... Discriminator Loss: 1.0226... Generator Loss: 1.2746
Epoch 2/2... Batch 670... Discriminator Loss: 1.1418... Generator Loss: 0.7758
Epoch 2/2... Batch 680... Discriminator Loss: 1.1074... Generator Loss: 1.1398
Epoch 2/2... Batch 690... Discriminator Loss: 1.0275... Generator Loss: 1.4793
Epoch 2/2... Batch 700... Discriminator Loss: 1.0233... Generator Loss: 1.2548</code></pre><p><img src="output_23_31.png" alt="png"></p>
<pre><code>Epoch 2/2... Batch 710... Discriminator Loss: 1.2469... Generator Loss: 0.7513
Epoch 2/2... Batch 720... Discriminator Loss: 0.9388... Generator Loss: 1.3823
Epoch 2/2... Batch 730... Discriminator Loss: 1.0916... Generator Loss: 1.0123
Epoch 2/2... Batch 740... Discriminator Loss: 0.8532... Generator Loss: 1.4688
Epoch 2/2... Batch 750... Discriminator Loss: 1.0012... Generator Loss: 1.1721
Epoch 2/2... Batch 760... Discriminator Loss: 0.9329... Generator Loss: 1.1667
Epoch 2/2... Batch 770... Discriminator Loss: 0.9331... Generator Loss: 1.2019
Epoch 2/2... Batch 780... Discriminator Loss: 1.0651... Generator Loss: 1.2122
Epoch 2/2... Batch 790... Discriminator Loss: 1.0660... Generator Loss: 0.9788
Epoch 2/2... Batch 800... Discriminator Loss: 1.0204... Generator Loss: 1.0103</code></pre><p><img src="output_23_33.png" alt="png"></p>
<pre><code>Epoch 2/2... Batch 810... Discriminator Loss: 0.9322... Generator Loss: 1.3958
Epoch 2/2... Batch 820... Discriminator Loss: 0.9664... Generator Loss: 1.3484
Epoch 2/2... Batch 830... Discriminator Loss: 0.9912... Generator Loss: 1.3500
Epoch 2/2... Batch 840... Discriminator Loss: 0.8798... Generator Loss: 1.5184
Epoch 2/2... Batch 850... Discriminator Loss: 0.9208... Generator Loss: 1.7758
Epoch 2/2... Batch 860... Discriminator Loss: 0.8183... Generator Loss: 1.5023
Epoch 2/2... Batch 870... Discriminator Loss: 1.1022... Generator Loss: 1.0484
Epoch 2/2... Batch 880... Discriminator Loss: 0.8392... Generator Loss: 1.3488
Epoch 2/2... Batch 890... Discriminator Loss: 0.9832... Generator Loss: 1.3378
Epoch 2/2... Batch 900... Discriminator Loss: 0.9039... Generator Loss: 1.4853</code></pre><p><img src="output_23_35.png" alt="png"></p>
<pre><code>Epoch 2/2... Batch 910... Discriminator Loss: 0.9838... Generator Loss: 1.1301
Epoch 2/2... Batch 920... Discriminator Loss: 0.8898... Generator Loss: 1.3110
Epoch 2/2... Batch 930... Discriminator Loss: 1.1253... Generator Loss: 1.0081</code></pre><h3 id="CelebA-1"><a href="#CelebA-1" class="headerlink" title="CelebA"></a>CelebA</h3><p>在 CelebA 上运行你的 GANs 模型。在一般的GPU上运行每次迭代大约需要 20 分钟。你可以运行整个迭代，或者当 GANs 开始产生真实人脸图像时停止它。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">z_dim = <span class="number">128</span></span><br><span class="line">learning_rate = <span class="number">0.0002</span></span><br><span class="line">beta1 = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">epochs = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">celeba_dataset = helper.Dataset(<span class="string">'celeba'</span>, glob(os.path.join(data_dir, <span class="string">'img_align_celeba/*.jpg'</span>)))</span><br><span class="line"><span class="keyword">with</span> tf.Graph().as_default():</span><br><span class="line">    train(epochs, batch_size, z_dim, learning_rate, beta1, celeba_dataset.get_batches,</span><br><span class="line">          celeba_dataset.shape, celeba_dataset.image_mode)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/1... Batch 10... Discriminator Loss: 0.9981... Generator Loss: 0.7771
Epoch 1/1... Batch 20... Discriminator Loss: 0.9016... Generator Loss: 0.8664
Epoch 1/1... Batch 30... Discriminator Loss: 0.7067... Generator Loss: 1.2069
Epoch 1/1... Batch 40... Discriminator Loss: 0.5190... Generator Loss: 1.9941
Epoch 1/1... Batch 50... Discriminator Loss: 0.5118... Generator Loss: 2.4666
Epoch 1/1... Batch 60... Discriminator Loss: 0.6421... Generator Loss: 4.1267
Epoch 1/1... Batch 70... Discriminator Loss: 0.7517... Generator Loss: 1.2524
Epoch 1/1... Batch 80... Discriminator Loss: 0.5839... Generator Loss: 2.3504
Epoch 1/1... Batch 90... Discriminator Loss: 0.5472... Generator Loss: 2.1309
Epoch 1/1... Batch 100... Discriminator Loss: 0.5191... Generator Loss: 2.4419</code></pre><p><img src="output_25_1.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 110... Discriminator Loss: 0.6108... Generator Loss: 2.5365
Epoch 1/1... Batch 120... Discriminator Loss: 0.7009... Generator Loss: 1.6806
Epoch 1/1... Batch 130... Discriminator Loss: 1.2114... Generator Loss: 2.1116
Epoch 1/1... Batch 140... Discriminator Loss: 0.8824... Generator Loss: 1.2442
Epoch 1/1... Batch 150... Discriminator Loss: 0.9787... Generator Loss: 1.0615
Epoch 1/1... Batch 160... Discriminator Loss: 0.9691... Generator Loss: 1.3978
Epoch 1/1... Batch 170... Discriminator Loss: 0.9818... Generator Loss: 1.1954
Epoch 1/1... Batch 180... Discriminator Loss: 0.9402... Generator Loss: 1.0828
Epoch 1/1... Batch 190... Discriminator Loss: 1.0999... Generator Loss: 1.2614
Epoch 1/1... Batch 200... Discriminator Loss: 1.2114... Generator Loss: 1.0437</code></pre><p><img src="output_25_3.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 210... Discriminator Loss: 1.0086... Generator Loss: 1.1933
Epoch 1/1... Batch 220... Discriminator Loss: 1.1822... Generator Loss: 0.9971
Epoch 1/1... Batch 230... Discriminator Loss: 1.0865... Generator Loss: 1.2736
Epoch 1/1... Batch 240... Discriminator Loss: 1.0688... Generator Loss: 1.1071
Epoch 1/1... Batch 250... Discriminator Loss: 0.9626... Generator Loss: 1.7346
Epoch 1/1... Batch 260... Discriminator Loss: 1.1661... Generator Loss: 0.9037
Epoch 1/1... Batch 270... Discriminator Loss: 0.9950... Generator Loss: 1.1802
Epoch 1/1... Batch 280... Discriminator Loss: 1.1150... Generator Loss: 0.9306
Epoch 1/1... Batch 290... Discriminator Loss: 0.9315... Generator Loss: 1.2671
Epoch 1/1... Batch 300... Discriminator Loss: 1.2516... Generator Loss: 0.9549</code></pre><p><img src="output_25_5.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 310... Discriminator Loss: 1.2923... Generator Loss: 0.7904
Epoch 1/1... Batch 320... Discriminator Loss: 0.9969... Generator Loss: 1.1599
Epoch 1/1... Batch 330... Discriminator Loss: 1.2161... Generator Loss: 0.8422
Epoch 1/1... Batch 340... Discriminator Loss: 1.1222... Generator Loss: 1.0251
Epoch 1/1... Batch 350... Discriminator Loss: 1.3111... Generator Loss: 0.7054
Epoch 1/1... Batch 360... Discriminator Loss: 1.2110... Generator Loss: 1.0047
Epoch 1/1... Batch 370... Discriminator Loss: 1.0935... Generator Loss: 1.0532
Epoch 1/1... Batch 380... Discriminator Loss: 1.2003... Generator Loss: 0.8454
Epoch 1/1... Batch 390... Discriminator Loss: 1.1732... Generator Loss: 0.8068
Epoch 1/1... Batch 400... Discriminator Loss: 1.1592... Generator Loss: 1.0911</code></pre><p><img src="output_25_7.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 410... Discriminator Loss: 1.1188... Generator Loss: 1.1321
Epoch 1/1... Batch 420... Discriminator Loss: 1.2830... Generator Loss: 0.8053
Epoch 1/1... Batch 430... Discriminator Loss: 1.0457... Generator Loss: 0.9653
Epoch 1/1... Batch 440... Discriminator Loss: 0.9826... Generator Loss: 1.2868
Epoch 1/1... Batch 450... Discriminator Loss: 1.5117... Generator Loss: 0.7326
Epoch 1/1... Batch 460... Discriminator Loss: 0.9725... Generator Loss: 1.0978
Epoch 1/1... Batch 470... Discriminator Loss: 1.4267... Generator Loss: 0.8671
Epoch 1/1... Batch 480... Discriminator Loss: 1.2267... Generator Loss: 0.9280
Epoch 1/1... Batch 490... Discriminator Loss: 1.1766... Generator Loss: 0.9417
Epoch 1/1... Batch 500... Discriminator Loss: 1.1857... Generator Loss: 0.7930</code></pre><p><img src="output_25_9.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 510... Discriminator Loss: 1.2500... Generator Loss: 0.9595
Epoch 1/1... Batch 520... Discriminator Loss: 1.2645... Generator Loss: 0.8823
Epoch 1/1... Batch 530... Discriminator Loss: 1.1794... Generator Loss: 0.9733
Epoch 1/1... Batch 540... Discriminator Loss: 1.4102... Generator Loss: 0.8095
Epoch 1/1... Batch 550... Discriminator Loss: 1.1388... Generator Loss: 0.9542
Epoch 1/1... Batch 560... Discriminator Loss: 1.1627... Generator Loss: 0.9303
Epoch 1/1... Batch 570... Discriminator Loss: 1.2920... Generator Loss: 0.9812
Epoch 1/1... Batch 580... Discriminator Loss: 1.2005... Generator Loss: 0.9594
Epoch 1/1... Batch 590... Discriminator Loss: 1.1743... Generator Loss: 0.9003
Epoch 1/1... Batch 600... Discriminator Loss: 1.2066... Generator Loss: 0.9200</code></pre><p><img src="output_25_11.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 610... Discriminator Loss: 1.2498... Generator Loss: 0.9885
Epoch 1/1... Batch 620... Discriminator Loss: 1.1849... Generator Loss: 1.0087
Epoch 1/1... Batch 630... Discriminator Loss: 1.1608... Generator Loss: 0.9816
Epoch 1/1... Batch 640... Discriminator Loss: 1.2275... Generator Loss: 0.9283
Epoch 1/1... Batch 650... Discriminator Loss: 1.1646... Generator Loss: 0.9433
Epoch 1/1... Batch 660... Discriminator Loss: 1.2177... Generator Loss: 0.9632
Epoch 1/1... Batch 670... Discriminator Loss: 1.3778... Generator Loss: 0.8534
Epoch 1/1... Batch 680... Discriminator Loss: 1.2189... Generator Loss: 0.8926
Epoch 1/1... Batch 690... Discriminator Loss: 1.4000... Generator Loss: 0.8340
Epoch 1/1... Batch 700... Discriminator Loss: 1.2425... Generator Loss: 0.8198</code></pre><p><img src="output_25_13.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 710... Discriminator Loss: 1.2259... Generator Loss: 0.8611
Epoch 1/1... Batch 720... Discriminator Loss: 1.2045... Generator Loss: 0.9232
Epoch 1/1... Batch 730... Discriminator Loss: 1.1803... Generator Loss: 0.9321
Epoch 1/1... Batch 740... Discriminator Loss: 1.2954... Generator Loss: 0.8308
Epoch 1/1... Batch 750... Discriminator Loss: 1.1874... Generator Loss: 1.0581
Epoch 1/1... Batch 760... Discriminator Loss: 1.3222... Generator Loss: 0.7852
Epoch 1/1... Batch 770... Discriminator Loss: 1.2697... Generator Loss: 0.9097
Epoch 1/1... Batch 780... Discriminator Loss: 1.2983... Generator Loss: 0.8731
Epoch 1/1... Batch 790... Discriminator Loss: 1.3264... Generator Loss: 0.7872
Epoch 1/1... Batch 800... Discriminator Loss: 1.2473... Generator Loss: 0.9322</code></pre><p><img src="output_25_15.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 810... Discriminator Loss: 1.2221... Generator Loss: 0.8978
Epoch 1/1... Batch 820... Discriminator Loss: 1.2596... Generator Loss: 0.8352
Epoch 1/1... Batch 830... Discriminator Loss: 1.2443... Generator Loss: 0.9121
Epoch 1/1... Batch 840... Discriminator Loss: 1.2686... Generator Loss: 0.9653
Epoch 1/1... Batch 850... Discriminator Loss: 1.3832... Generator Loss: 0.7721
Epoch 1/1... Batch 860... Discriminator Loss: 1.2008... Generator Loss: 0.8735
Epoch 1/1... Batch 870... Discriminator Loss: 1.2842... Generator Loss: 0.8377
Epoch 1/1... Batch 880... Discriminator Loss: 1.1266... Generator Loss: 0.9060
Epoch 1/1... Batch 890... Discriminator Loss: 1.2850... Generator Loss: 0.8653
Epoch 1/1... Batch 900... Discriminator Loss: 1.3255... Generator Loss: 0.8536</code></pre><p><img src="output_25_17.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 910... Discriminator Loss: 1.4506... Generator Loss: 0.6924
Epoch 1/1... Batch 920... Discriminator Loss: 1.2682... Generator Loss: 0.8340
Epoch 1/1... Batch 930... Discriminator Loss: 1.2845... Generator Loss: 0.8045
Epoch 1/1... Batch 940... Discriminator Loss: 1.3614... Generator Loss: 0.8227
Epoch 1/1... Batch 950... Discriminator Loss: 1.2370... Generator Loss: 0.8425
Epoch 1/1... Batch 960... Discriminator Loss: 1.1709... Generator Loss: 0.9615
Epoch 1/1... Batch 970... Discriminator Loss: 1.2701... Generator Loss: 0.8743
Epoch 1/1... Batch 980... Discriminator Loss: 1.2803... Generator Loss: 0.8770
Epoch 1/1... Batch 990... Discriminator Loss: 1.2514... Generator Loss: 0.9585
Epoch 1/1... Batch 1000... Discriminator Loss: 1.3493... Generator Loss: 0.7889</code></pre><p><img src="output_25_19.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 1010... Discriminator Loss: 1.2175... Generator Loss: 0.9166
Epoch 1/1... Batch 1020... Discriminator Loss: 1.3008... Generator Loss: 0.8842
Epoch 1/1... Batch 1030... Discriminator Loss: 1.2665... Generator Loss: 0.9152
Epoch 1/1... Batch 1040... Discriminator Loss: 1.2192... Generator Loss: 0.9136
Epoch 1/1... Batch 1050... Discriminator Loss: 1.2416... Generator Loss: 0.8538
Epoch 1/1... Batch 1060... Discriminator Loss: 1.2799... Generator Loss: 0.8829
Epoch 1/1... Batch 1070... Discriminator Loss: 1.2771... Generator Loss: 0.8186
Epoch 1/1... Batch 1080... Discriminator Loss: 1.2694... Generator Loss: 0.9232
Epoch 1/1... Batch 1090... Discriminator Loss: 1.3148... Generator Loss: 0.8158
Epoch 1/1... Batch 1100... Discriminator Loss: 1.2851... Generator Loss: 0.8493</code></pre><p><img src="output_25_21.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 1110... Discriminator Loss: 1.3952... Generator Loss: 0.7836
Epoch 1/1... Batch 1120... Discriminator Loss: 1.2444... Generator Loss: 0.8344
Epoch 1/1... Batch 1130... Discriminator Loss: 1.2850... Generator Loss: 0.9236
Epoch 1/1... Batch 1140... Discriminator Loss: 1.3081... Generator Loss: 0.8108
Epoch 1/1... Batch 1150... Discriminator Loss: 1.3350... Generator Loss: 0.7956
Epoch 1/1... Batch 1160... Discriminator Loss: 1.2242... Generator Loss: 0.9067
Epoch 1/1... Batch 1170... Discriminator Loss: 1.2988... Generator Loss: 0.8518
Epoch 1/1... Batch 1180... Discriminator Loss: 1.2646... Generator Loss: 0.8700
Epoch 1/1... Batch 1190... Discriminator Loss: 1.2182... Generator Loss: 0.8550
Epoch 1/1... Batch 1200... Discriminator Loss: 1.2484... Generator Loss: 0.9214</code></pre><p><img src="output_25_23.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 1210... Discriminator Loss: 1.3718... Generator Loss: 0.8059
Epoch 1/1... Batch 1220... Discriminator Loss: 1.2959... Generator Loss: 0.8333
Epoch 1/1... Batch 1230... Discriminator Loss: 1.2722... Generator Loss: 0.9051
Epoch 1/1... Batch 1240... Discriminator Loss: 1.2833... Generator Loss: 0.8142
Epoch 1/1... Batch 1250... Discriminator Loss: 1.2560... Generator Loss: 0.8072
Epoch 1/1... Batch 1260... Discriminator Loss: 1.3153... Generator Loss: 0.8065
Epoch 1/1... Batch 1270... Discriminator Loss: 1.3130... Generator Loss: 0.8006
Epoch 1/1... Batch 1280... Discriminator Loss: 1.2618... Generator Loss: 0.8966
Epoch 1/1... Batch 1290... Discriminator Loss: 1.2786... Generator Loss: 0.8311
Epoch 1/1... Batch 1300... Discriminator Loss: 1.2449... Generator Loss: 0.8881</code></pre><p><img src="output_25_25.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 1310... Discriminator Loss: 1.2607... Generator Loss: 0.8691
Epoch 1/1... Batch 1320... Discriminator Loss: 1.3255... Generator Loss: 0.8364
Epoch 1/1... Batch 1330... Discriminator Loss: 1.3190... Generator Loss: 0.8511
Epoch 1/1... Batch 1340... Discriminator Loss: 1.3341... Generator Loss: 0.8172
Epoch 1/1... Batch 1350... Discriminator Loss: 1.3062... Generator Loss: 0.7934
Epoch 1/1... Batch 1360... Discriminator Loss: 1.2737... Generator Loss: 0.8675
Epoch 1/1... Batch 1370... Discriminator Loss: 1.4014... Generator Loss: 0.7046
Epoch 1/1... Batch 1380... Discriminator Loss: 1.3793... Generator Loss: 0.7934
Epoch 1/1... Batch 1390... Discriminator Loss: 1.2920... Generator Loss: 0.7808
Epoch 1/1... Batch 1400... Discriminator Loss: 1.2964... Generator Loss: 0.8559</code></pre><p><img src="output_25_27.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 1410... Discriminator Loss: 1.3307... Generator Loss: 0.8587
Epoch 1/1... Batch 1420... Discriminator Loss: 1.1907... Generator Loss: 0.9548
Epoch 1/1... Batch 1430... Discriminator Loss: 1.2613... Generator Loss: 0.8367
Epoch 1/1... Batch 1440... Discriminator Loss: 1.2975... Generator Loss: 0.8297
Epoch 1/1... Batch 1450... Discriminator Loss: 1.4721... Generator Loss: 0.8021
Epoch 1/1... Batch 1460... Discriminator Loss: 1.3033... Generator Loss: 0.8686
Epoch 1/1... Batch 1470... Discriminator Loss: 1.2740... Generator Loss: 0.9183
Epoch 1/1... Batch 1480... Discriminator Loss: 1.2828... Generator Loss: 0.8822
Epoch 1/1... Batch 1490... Discriminator Loss: 1.3281... Generator Loss: 0.7889
Epoch 1/1... Batch 1500... Discriminator Loss: 1.4039... Generator Loss: 0.7901</code></pre><p><img src="output_25_29.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 1510... Discriminator Loss: 1.3994... Generator Loss: 0.8115
Epoch 1/1... Batch 1520... Discriminator Loss: 1.3938... Generator Loss: 0.8660
Epoch 1/1... Batch 1530... Discriminator Loss: 1.3472... Generator Loss: 0.8626
Epoch 1/1... Batch 1540... Discriminator Loss: 1.2848... Generator Loss: 0.8554
Epoch 1/1... Batch 1550... Discriminator Loss: 1.2706... Generator Loss: 0.8981
Epoch 1/1... Batch 1560... Discriminator Loss: 1.3040... Generator Loss: 0.8787
Epoch 1/1... Batch 1570... Discriminator Loss: 1.2928... Generator Loss: 0.8965
Epoch 1/1... Batch 1580... Discriminator Loss: 1.4242... Generator Loss: 0.8227
Epoch 1/1... Batch 1590... Discriminator Loss: 1.3095... Generator Loss: 0.8228
Epoch 1/1... Batch 1600... Discriminator Loss: 1.3139... Generator Loss: 0.8297</code></pre><p><img src="output_25_31.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 1610... Discriminator Loss: 1.3187... Generator Loss: 0.8088
Epoch 1/1... Batch 1620... Discriminator Loss: 1.1701... Generator Loss: 0.9060
Epoch 1/1... Batch 1630... Discriminator Loss: 1.2032... Generator Loss: 0.9128
Epoch 1/1... Batch 1640... Discriminator Loss: 1.3423... Generator Loss: 0.8544
Epoch 1/1... Batch 1650... Discriminator Loss: 1.2836... Generator Loss: 0.8374
Epoch 1/1... Batch 1660... Discriminator Loss: 1.3496... Generator Loss: 0.8617
Epoch 1/1... Batch 1670... Discriminator Loss: 1.3002... Generator Loss: 0.8886
Epoch 1/1... Batch 1680... Discriminator Loss: 1.3113... Generator Loss: 0.8682
Epoch 1/1... Batch 1690... Discriminator Loss: 1.3523... Generator Loss: 0.8381
Epoch 1/1... Batch 1700... Discriminator Loss: 1.3383... Generator Loss: 0.8266</code></pre><p><img src="output_25_33.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 1710... Discriminator Loss: 1.2662... Generator Loss: 0.8938
Epoch 1/1... Batch 1720... Discriminator Loss: 1.3351... Generator Loss: 0.8087
Epoch 1/1... Batch 1730... Discriminator Loss: 1.3067... Generator Loss: 0.8960
Epoch 1/1... Batch 1740... Discriminator Loss: 1.3567... Generator Loss: 0.7756
Epoch 1/1... Batch 1750... Discriminator Loss: 1.4145... Generator Loss: 0.8081
Epoch 1/1... Batch 1760... Discriminator Loss: 1.2260... Generator Loss: 0.9472
Epoch 1/1... Batch 1770... Discriminator Loss: 1.2291... Generator Loss: 0.9573
Epoch 1/1... Batch 1780... Discriminator Loss: 1.3257... Generator Loss: 0.8390
Epoch 1/1... Batch 1790... Discriminator Loss: 1.2925... Generator Loss: 0.9057
Epoch 1/1... Batch 1800... Discriminator Loss: 1.3113... Generator Loss: 0.7977</code></pre><p><img src="output_25_35.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 1810... Discriminator Loss: 1.2760... Generator Loss: 0.8303
Epoch 1/1... Batch 1820... Discriminator Loss: 1.2022... Generator Loss: 0.8662
Epoch 1/1... Batch 1830... Discriminator Loss: 1.2934... Generator Loss: 0.7599
Epoch 1/1... Batch 1840... Discriminator Loss: 1.3896... Generator Loss: 0.7332
Epoch 1/1... Batch 1850... Discriminator Loss: 1.3435... Generator Loss: 0.7790
Epoch 1/1... Batch 1860... Discriminator Loss: 1.2905... Generator Loss: 0.8599
Epoch 1/1... Batch 1870... Discriminator Loss: 1.3372... Generator Loss: 0.8194
Epoch 1/1... Batch 1880... Discriminator Loss: 1.3292... Generator Loss: 0.8563
Epoch 1/1... Batch 1890... Discriminator Loss: 1.2922... Generator Loss: 0.8493
Epoch 1/1... Batch 1900... Discriminator Loss: 1.3670... Generator Loss: 0.9143</code></pre><p><img src="output_25_37.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 1910... Discriminator Loss: 1.2610... Generator Loss: 0.8383
Epoch 1/1... Batch 1920... Discriminator Loss: 1.2835... Generator Loss: 0.8211
Epoch 1/1... Batch 1930... Discriminator Loss: 1.3084... Generator Loss: 0.7800
Epoch 1/1... Batch 1940... Discriminator Loss: 1.3329... Generator Loss: 0.8437
Epoch 1/1... Batch 1950... Discriminator Loss: 1.4528... Generator Loss: 0.7125
Epoch 1/1... Batch 1960... Discriminator Loss: 1.3187... Generator Loss: 0.8177
Epoch 1/1... Batch 1970... Discriminator Loss: 1.2423... Generator Loss: 0.9067
Epoch 1/1... Batch 1980... Discriminator Loss: 1.3378... Generator Loss: 0.7983
Epoch 1/1... Batch 1990... Discriminator Loss: 1.3147... Generator Loss: 0.8135
Epoch 1/1... Batch 2000... Discriminator Loss: 1.2507... Generator Loss: 0.8298</code></pre><p><img src="output_25_39.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 2010... Discriminator Loss: 1.3365... Generator Loss: 0.8947
Epoch 1/1... Batch 2020... Discriminator Loss: 1.2580... Generator Loss: 0.8540
Epoch 1/1... Batch 2030... Discriminator Loss: 1.2834... Generator Loss: 0.8756
Epoch 1/1... Batch 2040... Discriminator Loss: 1.3726... Generator Loss: 0.7878
Epoch 1/1... Batch 2050... Discriminator Loss: 1.3191... Generator Loss: 0.8491
Epoch 1/1... Batch 2060... Discriminator Loss: 1.3068... Generator Loss: 0.8496
Epoch 1/1... Batch 2070... Discriminator Loss: 1.3405... Generator Loss: 0.8006
Epoch 1/1... Batch 2080... Discriminator Loss: 1.3170... Generator Loss: 0.8117
Epoch 1/1... Batch 2090... Discriminator Loss: 1.3434... Generator Loss: 0.8363
Epoch 1/1... Batch 2100... Discriminator Loss: 1.3048... Generator Loss: 0.8710</code></pre><p><img src="output_25_41.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 2110... Discriminator Loss: 1.2579... Generator Loss: 0.9008
Epoch 1/1... Batch 2120... Discriminator Loss: 1.3002... Generator Loss: 0.8223
Epoch 1/1... Batch 2130... Discriminator Loss: 1.3369... Generator Loss: 0.7678
Epoch 1/1... Batch 2140... Discriminator Loss: 1.4217... Generator Loss: 0.7495
Epoch 1/1... Batch 2150... Discriminator Loss: 1.2846... Generator Loss: 0.8571
Epoch 1/1... Batch 2160... Discriminator Loss: 1.2974... Generator Loss: 0.8260
Epoch 1/1... Batch 2170... Discriminator Loss: 1.3440... Generator Loss: 0.7584
Epoch 1/1... Batch 2180... Discriminator Loss: 1.2794... Generator Loss: 0.8621
Epoch 1/1... Batch 2190... Discriminator Loss: 1.3800... Generator Loss: 0.8225
Epoch 1/1... Batch 2200... Discriminator Loss: 1.3725... Generator Loss: 0.8348</code></pre><p><img src="output_25_43.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 2210... Discriminator Loss: 1.4011... Generator Loss: 0.8716
Epoch 1/1... Batch 2220... Discriminator Loss: 1.2680... Generator Loss: 0.7884
Epoch 1/1... Batch 2230... Discriminator Loss: 1.3279... Generator Loss: 0.8788
Epoch 1/1... Batch 2240... Discriminator Loss: 1.3336... Generator Loss: 0.8579
Epoch 1/1... Batch 2250... Discriminator Loss: 1.3311... Generator Loss: 0.8104
Epoch 1/1... Batch 2260... Discriminator Loss: 1.2884... Generator Loss: 0.8263
Epoch 1/1... Batch 2270... Discriminator Loss: 1.3730... Generator Loss: 0.8092
Epoch 1/1... Batch 2280... Discriminator Loss: 1.1840... Generator Loss: 0.8487
Epoch 1/1... Batch 2290... Discriminator Loss: 1.3045... Generator Loss: 0.7983
Epoch 1/1... Batch 2300... Discriminator Loss: 1.3425... Generator Loss: 0.7484</code></pre><p><img src="output_25_45.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 2310... Discriminator Loss: 1.3543... Generator Loss: 0.8415
Epoch 1/1... Batch 2320... Discriminator Loss: 1.3897... Generator Loss: 0.8340
Epoch 1/1... Batch 2330... Discriminator Loss: 1.3158... Generator Loss: 0.7736
Epoch 1/1... Batch 2340... Discriminator Loss: 1.3775... Generator Loss: 0.8433
Epoch 1/1... Batch 2350... Discriminator Loss: 1.3236... Generator Loss: 0.8590
Epoch 1/1... Batch 2360... Discriminator Loss: 1.3130... Generator Loss: 0.8142
Epoch 1/1... Batch 2370... Discriminator Loss: 1.3289... Generator Loss: 0.8560
Epoch 1/1... Batch 2380... Discriminator Loss: 1.4008... Generator Loss: 0.8244
Epoch 1/1... Batch 2390... Discriminator Loss: 1.2796... Generator Loss: 0.8480
Epoch 1/1... Batch 2400... Discriminator Loss: 1.2258... Generator Loss: 0.8496</code></pre><p><img src="output_25_47.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 2410... Discriminator Loss: 1.3604... Generator Loss: 0.8419
Epoch 1/1... Batch 2420... Discriminator Loss: 1.3860... Generator Loss: 0.8519
Epoch 1/1... Batch 2430... Discriminator Loss: 1.2405... Generator Loss: 0.8714
Epoch 1/1... Batch 2440... Discriminator Loss: 1.3507... Generator Loss: 0.8708
Epoch 1/1... Batch 2450... Discriminator Loss: 1.2694... Generator Loss: 0.8623
Epoch 1/1... Batch 2460... Discriminator Loss: 1.3077... Generator Loss: 0.8747
Epoch 1/1... Batch 2470... Discriminator Loss: 1.3550... Generator Loss: 0.7979
Epoch 1/1... Batch 2480... Discriminator Loss: 1.2798... Generator Loss: 0.9670
Epoch 1/1... Batch 2490... Discriminator Loss: 1.4202... Generator Loss: 0.7889
Epoch 1/1... Batch 2500... Discriminator Loss: 1.3324... Generator Loss: 0.8227</code></pre><p><img src="output_25_49.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 2510... Discriminator Loss: 1.2895... Generator Loss: 0.8955
Epoch 1/1... Batch 2520... Discriminator Loss: 1.2875... Generator Loss: 0.8465
Epoch 1/1... Batch 2530... Discriminator Loss: 1.3957... Generator Loss: 0.8241
Epoch 1/1... Batch 2540... Discriminator Loss: 1.3018... Generator Loss: 0.9072
Epoch 1/1... Batch 2550... Discriminator Loss: 1.2491... Generator Loss: 0.8656
Epoch 1/1... Batch 2560... Discriminator Loss: 1.3848... Generator Loss: 0.8276
Epoch 1/1... Batch 2570... Discriminator Loss: 1.3409... Generator Loss: 0.7581
Epoch 1/1... Batch 2580... Discriminator Loss: 1.3751... Generator Loss: 0.7807
Epoch 1/1... Batch 2590... Discriminator Loss: 1.2439... Generator Loss: 0.9024
Epoch 1/1... Batch 2600... Discriminator Loss: 1.2971... Generator Loss: 0.8984</code></pre><p><img src="output_25_51.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 2610... Discriminator Loss: 1.2999... Generator Loss: 0.8598
Epoch 1/1... Batch 2620... Discriminator Loss: 1.2436... Generator Loss: 0.8827
Epoch 1/1... Batch 2630... Discriminator Loss: 1.3084... Generator Loss: 0.8549
Epoch 1/1... Batch 2640... Discriminator Loss: 1.2893... Generator Loss: 0.8136
Epoch 1/1... Batch 2650... Discriminator Loss: 1.2818... Generator Loss: 0.8204
Epoch 1/1... Batch 2660... Discriminator Loss: 1.3016... Generator Loss: 0.8552
Epoch 1/1... Batch 2670... Discriminator Loss: 1.3060... Generator Loss: 0.7911
Epoch 1/1... Batch 2680... Discriminator Loss: 1.3459... Generator Loss: 0.7951
Epoch 1/1... Batch 2690... Discriminator Loss: 1.2889... Generator Loss: 0.8255
Epoch 1/1... Batch 2700... Discriminator Loss: 1.3028... Generator Loss: 0.8365</code></pre><p><img src="output_25_53.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 2710... Discriminator Loss: 1.3038... Generator Loss: 0.8372
Epoch 1/1... Batch 2720... Discriminator Loss: 1.3204... Generator Loss: 0.8535
Epoch 1/1... Batch 2730... Discriminator Loss: 1.3587... Generator Loss: 0.7882
Epoch 1/1... Batch 2740... Discriminator Loss: 1.2938... Generator Loss: 0.8357
Epoch 1/1... Batch 2750... Discriminator Loss: 1.2775... Generator Loss: 0.8302
Epoch 1/1... Batch 2760... Discriminator Loss: 1.3236... Generator Loss: 0.8041
Epoch 1/1... Batch 2770... Discriminator Loss: 1.3390... Generator Loss: 0.7893
Epoch 1/1... Batch 2780... Discriminator Loss: 1.2987... Generator Loss: 0.8412
Epoch 1/1... Batch 2790... Discriminator Loss: 1.3107... Generator Loss: 0.8596
Epoch 1/1... Batch 2800... Discriminator Loss: 1.2503... Generator Loss: 0.9872</code></pre><p><img src="output_25_55.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 2810... Discriminator Loss: 1.2915... Generator Loss: 0.8449
Epoch 1/1... Batch 2820... Discriminator Loss: 1.3148... Generator Loss: 0.8897
Epoch 1/1... Batch 2830... Discriminator Loss: 1.3492... Generator Loss: 0.7679
Epoch 1/1... Batch 2840... Discriminator Loss: 1.2644... Generator Loss: 0.8366
Epoch 1/1... Batch 2850... Discriminator Loss: 1.2778... Generator Loss: 0.8532
Epoch 1/1... Batch 2860... Discriminator Loss: 1.3265... Generator Loss: 0.8507
Epoch 1/1... Batch 2870... Discriminator Loss: 1.3529... Generator Loss: 0.7967
Epoch 1/1... Batch 2880... Discriminator Loss: 1.3491... Generator Loss: 0.7955
Epoch 1/1... Batch 2890... Discriminator Loss: 1.3173... Generator Loss: 0.8044
Epoch 1/1... Batch 2900... Discriminator Loss: 1.3518... Generator Loss: 0.8273</code></pre><p><img src="output_25_57.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 2910... Discriminator Loss: 1.3295... Generator Loss: 0.8063
Epoch 1/1... Batch 2920... Discriminator Loss: 1.3080... Generator Loss: 0.8677
Epoch 1/1... Batch 2930... Discriminator Loss: 1.2582... Generator Loss: 0.8968
Epoch 1/1... Batch 2940... Discriminator Loss: 1.3414... Generator Loss: 0.8523
Epoch 1/1... Batch 2950... Discriminator Loss: 1.2716... Generator Loss: 0.9375
Epoch 1/1... Batch 2960... Discriminator Loss: 1.3844... Generator Loss: 0.7941
Epoch 1/1... Batch 2970... Discriminator Loss: 1.2301... Generator Loss: 0.8791
Epoch 1/1... Batch 2980... Discriminator Loss: 1.3126... Generator Loss: 0.8178
Epoch 1/1... Batch 2990... Discriminator Loss: 1.1923... Generator Loss: 0.9349
Epoch 1/1... Batch 3000... Discriminator Loss: 1.3141... Generator Loss: 0.8358</code></pre><p><img src="output_25_59.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 3010... Discriminator Loss: 1.3868... Generator Loss: 0.7965
Epoch 1/1... Batch 3020... Discriminator Loss: 1.3403... Generator Loss: 0.7965
Epoch 1/1... Batch 3030... Discriminator Loss: 1.3207... Generator Loss: 0.8411
Epoch 1/1... Batch 3040... Discriminator Loss: 1.3025... Generator Loss: 0.8382
Epoch 1/1... Batch 3050... Discriminator Loss: 1.3255... Generator Loss: 0.8690
Epoch 1/1... Batch 3060... Discriminator Loss: 1.2922... Generator Loss: 0.9095
Epoch 1/1... Batch 3070... Discriminator Loss: 1.3520... Generator Loss: 0.8327
Epoch 1/1... Batch 3080... Discriminator Loss: 1.2829... Generator Loss: 0.8751
Epoch 1/1... Batch 3090... Discriminator Loss: 1.3533... Generator Loss: 0.8036
Epoch 1/1... Batch 3100... Discriminator Loss: 1.3917... Generator Loss: 0.7761</code></pre><p><img src="output_25_61.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 3110... Discriminator Loss: 1.3683... Generator Loss: 0.8178
Epoch 1/1... Batch 3120... Discriminator Loss: 1.3576... Generator Loss: 0.8351
Epoch 1/1... Batch 3130... Discriminator Loss: 1.2910... Generator Loss: 0.8652
Epoch 1/1... Batch 3140... Discriminator Loss: 1.3098... Generator Loss: 0.8024
Epoch 1/1... Batch 3150... Discriminator Loss: 1.2481... Generator Loss: 0.8237
Epoch 1/1... Batch 3160... Discriminator Loss: 1.2747... Generator Loss: 0.8618
Epoch 1/1... Batch 3170... Discriminator Loss: 1.3122... Generator Loss: 0.7966
Epoch 1/1... Batch 3180... Discriminator Loss: 1.4232... Generator Loss: 0.7593
Epoch 1/1... Batch 3190... Discriminator Loss: 1.3122... Generator Loss: 0.8337
Epoch 1/1... Batch 3200... Discriminator Loss: 1.3739... Generator Loss: 0.7862</code></pre><p><img src="output_25_63.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 3210... Discriminator Loss: 1.2702... Generator Loss: 0.9017
Epoch 1/1... Batch 3220... Discriminator Loss: 1.3077... Generator Loss: 0.8475
Epoch 1/1... Batch 3230... Discriminator Loss: 1.2826... Generator Loss: 0.8715
Epoch 1/1... Batch 3240... Discriminator Loss: 1.3612... Generator Loss: 0.7924
Epoch 1/1... Batch 3250... Discriminator Loss: 1.2673... Generator Loss: 0.8411
Epoch 1/1... Batch 3260... Discriminator Loss: 1.2608... Generator Loss: 0.9819
Epoch 1/1... Batch 3270... Discriminator Loss: 1.3917... Generator Loss: 0.7695
Epoch 1/1... Batch 3280... Discriminator Loss: 1.3742... Generator Loss: 0.8342
Epoch 1/1... Batch 3290... Discriminator Loss: 1.3867... Generator Loss: 0.7643
Epoch 1/1... Batch 3300... Discriminator Loss: 1.2463... Generator Loss: 0.8322</code></pre><p><img src="output_25_65.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 3310... Discriminator Loss: 1.3915... Generator Loss: 0.8278
Epoch 1/1... Batch 3320... Discriminator Loss: 1.3851... Generator Loss: 0.7707
Epoch 1/1... Batch 3330... Discriminator Loss: 1.3772... Generator Loss: 0.7792
Epoch 1/1... Batch 3340... Discriminator Loss: 1.3331... Generator Loss: 0.7878
Epoch 1/1... Batch 3350... Discriminator Loss: 1.3180... Generator Loss: 0.7574
Epoch 1/1... Batch 3360... Discriminator Loss: 1.3590... Generator Loss: 0.7860
Epoch 1/1... Batch 3370... Discriminator Loss: 1.2984... Generator Loss: 0.7365
Epoch 1/1... Batch 3380... Discriminator Loss: 1.3231... Generator Loss: 0.9058
Epoch 1/1... Batch 3390... Discriminator Loss: 1.2917... Generator Loss: 0.8435
Epoch 1/1... Batch 3400... Discriminator Loss: 1.2712... Generator Loss: 0.8732</code></pre><p><img src="output_25_67.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 3410... Discriminator Loss: 1.2184... Generator Loss: 0.9084
Epoch 1/1... Batch 3420... Discriminator Loss: 1.3361... Generator Loss: 0.8649
Epoch 1/1... Batch 3430... Discriminator Loss: 1.2314... Generator Loss: 0.8647
Epoch 1/1... Batch 3440... Discriminator Loss: 1.3229... Generator Loss: 0.7710
Epoch 1/1... Batch 3450... Discriminator Loss: 1.3640... Generator Loss: 0.7601
Epoch 1/1... Batch 3460... Discriminator Loss: 1.2722... Generator Loss: 0.8593
Epoch 1/1... Batch 3470... Discriminator Loss: 1.3355... Generator Loss: 0.8038
Epoch 1/1... Batch 3480... Discriminator Loss: 1.3809... Generator Loss: 0.7951
Epoch 1/1... Batch 3490... Discriminator Loss: 1.3439... Generator Loss: 0.8021
Epoch 1/1... Batch 3500... Discriminator Loss: 1.3409... Generator Loss: 0.8149</code></pre><p><img src="output_25_69.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 3510... Discriminator Loss: 1.2773... Generator Loss: 0.7827
Epoch 1/1... Batch 3520... Discriminator Loss: 1.3282... Generator Loss: 0.8164
Epoch 1/1... Batch 3530... Discriminator Loss: 1.3193... Generator Loss: 0.8273
Epoch 1/1... Batch 3540... Discriminator Loss: 1.2965... Generator Loss: 0.8386
Epoch 1/1... Batch 3550... Discriminator Loss: 1.3552... Generator Loss: 0.7619
Epoch 1/1... Batch 3560... Discriminator Loss: 1.3685... Generator Loss: 0.7408
Epoch 1/1... Batch 3570... Discriminator Loss: 1.3477... Generator Loss: 0.7286
Epoch 1/1... Batch 3580... Discriminator Loss: 1.3254... Generator Loss: 0.7952
Epoch 1/1... Batch 3590... Discriminator Loss: 1.2705... Generator Loss: 0.9073
Epoch 1/1... Batch 3600... Discriminator Loss: 1.2291... Generator Loss: 0.7909</code></pre><p><img src="output_25_71.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 3610... Discriminator Loss: 1.3415... Generator Loss: 0.7891
Epoch 1/1... Batch 3620... Discriminator Loss: 1.3242... Generator Loss: 0.8152
Epoch 1/1... Batch 3630... Discriminator Loss: 1.3095... Generator Loss: 0.8825
Epoch 1/1... Batch 3640... Discriminator Loss: 1.2908... Generator Loss: 0.8037
Epoch 1/1... Batch 3650... Discriminator Loss: 1.1980... Generator Loss: 0.8652
Epoch 1/1... Batch 3660... Discriminator Loss: 1.2996... Generator Loss: 0.8290
Epoch 1/1... Batch 3670... Discriminator Loss: 1.3239... Generator Loss: 0.8954
Epoch 1/1... Batch 3680... Discriminator Loss: 1.3265... Generator Loss: 0.8133
Epoch 1/1... Batch 3690... Discriminator Loss: 1.3589... Generator Loss: 0.8366
Epoch 1/1... Batch 3700... Discriminator Loss: 1.3285... Generator Loss: 0.7586</code></pre><p><img src="output_25_73.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 3710... Discriminator Loss: 1.3426... Generator Loss: 0.7970
Epoch 1/1... Batch 3720... Discriminator Loss: 1.3014... Generator Loss: 0.8132
Epoch 1/1... Batch 3730... Discriminator Loss: 1.2428... Generator Loss: 0.8893
Epoch 1/1... Batch 3740... Discriminator Loss: 1.3326... Generator Loss: 0.8080
Epoch 1/1... Batch 3750... Discriminator Loss: 1.3782... Generator Loss: 0.7452
Epoch 1/1... Batch 3760... Discriminator Loss: 1.3101... Generator Loss: 0.8193
Epoch 1/1... Batch 3770... Discriminator Loss: 1.3348... Generator Loss: 0.8098
Epoch 1/1... Batch 3780... Discriminator Loss: 1.2942... Generator Loss: 0.8319
Epoch 1/1... Batch 3790... Discriminator Loss: 1.3768... Generator Loss: 0.8176
Epoch 1/1... Batch 3800... Discriminator Loss: 1.3896... Generator Loss: 0.7380</code></pre><p><img src="output_25_75.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 3810... Discriminator Loss: 1.3274... Generator Loss: 0.8053
Epoch 1/1... Batch 3820... Discriminator Loss: 1.2823... Generator Loss: 0.8184
Epoch 1/1... Batch 3830... Discriminator Loss: 1.2471... Generator Loss: 0.8104
Epoch 1/1... Batch 3840... Discriminator Loss: 1.2882... Generator Loss: 0.8234
Epoch 1/1... Batch 3850... Discriminator Loss: 1.3539... Generator Loss: 0.8503
Epoch 1/1... Batch 3860... Discriminator Loss: 1.3514... Generator Loss: 0.7677
Epoch 1/1... Batch 3870... Discriminator Loss: 1.3691... Generator Loss: 0.7664
Epoch 1/1... Batch 3880... Discriminator Loss: 1.2723... Generator Loss: 0.8292
Epoch 1/1... Batch 3890... Discriminator Loss: 1.2938... Generator Loss: 0.8485
Epoch 1/1... Batch 3900... Discriminator Loss: 1.2657... Generator Loss: 0.8290</code></pre><p><img src="output_25_77.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 3910... Discriminator Loss: 1.3379... Generator Loss: 0.8177
Epoch 1/1... Batch 3920... Discriminator Loss: 1.3608... Generator Loss: 0.7615
Epoch 1/1... Batch 3930... Discriminator Loss: 1.2909... Generator Loss: 0.8396
Epoch 1/1... Batch 3940... Discriminator Loss: 1.2430... Generator Loss: 0.8836
Epoch 1/1... Batch 3950... Discriminator Loss: 1.3357... Generator Loss: 0.7869
Epoch 1/1... Batch 3960... Discriminator Loss: 1.3905... Generator Loss: 0.7490
Epoch 1/1... Batch 3970... Discriminator Loss: 1.2970... Generator Loss: 0.8401
Epoch 1/1... Batch 3980... Discriminator Loss: 1.3955... Generator Loss: 0.8246
Epoch 1/1... Batch 3990... Discriminator Loss: 1.2858... Generator Loss: 0.8763
Epoch 1/1... Batch 4000... Discriminator Loss: 1.3529... Generator Loss: 0.7891</code></pre><p><img src="output_25_79.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 4010... Discriminator Loss: 1.4170... Generator Loss: 0.7812
Epoch 1/1... Batch 4020... Discriminator Loss: 1.3103... Generator Loss: 0.8150
Epoch 1/1... Batch 4030... Discriminator Loss: 1.3286... Generator Loss: 0.8003
Epoch 1/1... Batch 4040... Discriminator Loss: 1.3047... Generator Loss: 0.8365
Epoch 1/1... Batch 4050... Discriminator Loss: 1.3102... Generator Loss: 0.8114
Epoch 1/1... Batch 4060... Discriminator Loss: 1.2617... Generator Loss: 0.8055
Epoch 1/1... Batch 4070... Discriminator Loss: 1.3449... Generator Loss: 0.7941
Epoch 1/1... Batch 4080... Discriminator Loss: 1.2300... Generator Loss: 0.8639
Epoch 1/1... Batch 4090... Discriminator Loss: 1.2184... Generator Loss: 0.8178
Epoch 1/1... Batch 4100... Discriminator Loss: 1.2310... Generator Loss: 0.8800</code></pre><p><img src="output_25_81.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 4110... Discriminator Loss: 1.3564... Generator Loss: 0.8671
Epoch 1/1... Batch 4120... Discriminator Loss: 1.3549... Generator Loss: 0.7691
Epoch 1/1... Batch 4130... Discriminator Loss: 1.3613... Generator Loss: 0.7653
Epoch 1/1... Batch 4140... Discriminator Loss: 1.2941... Generator Loss: 0.8455
Epoch 1/1... Batch 4150... Discriminator Loss: 1.3126... Generator Loss: 0.9079
Epoch 1/1... Batch 4160... Discriminator Loss: 1.3377... Generator Loss: 0.7801
Epoch 1/1... Batch 4170... Discriminator Loss: 1.2799... Generator Loss: 0.8961
Epoch 1/1... Batch 4180... Discriminator Loss: 1.3762... Generator Loss: 0.7947
Epoch 1/1... Batch 4190... Discriminator Loss: 1.2743... Generator Loss: 0.8223
Epoch 1/1... Batch 4200... Discriminator Loss: 1.4002... Generator Loss: 0.7400</code></pre><p><img src="output_25_83.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 4210... Discriminator Loss: 1.3046... Generator Loss: 0.7753
Epoch 1/1... Batch 4220... Discriminator Loss: 1.3160... Generator Loss: 0.7524
Epoch 1/1... Batch 4230... Discriminator Loss: 1.2531... Generator Loss: 0.8358
Epoch 1/1... Batch 4240... Discriminator Loss: 1.3581... Generator Loss: 0.8126
Epoch 1/1... Batch 4250... Discriminator Loss: 1.3100... Generator Loss: 0.8528
Epoch 1/1... Batch 4260... Discriminator Loss: 1.3058... Generator Loss: 0.8352
Epoch 1/1... Batch 4270... Discriminator Loss: 1.2798... Generator Loss: 0.8269
Epoch 1/1... Batch 4280... Discriminator Loss: 1.2786... Generator Loss: 0.8031
Epoch 1/1... Batch 4290... Discriminator Loss: 1.3932... Generator Loss: 0.7881
Epoch 1/1... Batch 4300... Discriminator Loss: 1.3067... Generator Loss: 0.8738</code></pre><p><img src="output_25_85.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 4310... Discriminator Loss: 1.2686... Generator Loss: 0.8054
Epoch 1/1... Batch 4320... Discriminator Loss: 1.2720... Generator Loss: 0.8825
Epoch 1/1... Batch 4330... Discriminator Loss: 1.2665... Generator Loss: 0.9282
Epoch 1/1... Batch 4340... Discriminator Loss: 1.2414... Generator Loss: 0.8806
Epoch 1/1... Batch 4350... Discriminator Loss: 1.4383... Generator Loss: 0.7535
Epoch 1/1... Batch 4360... Discriminator Loss: 1.3076... Generator Loss: 0.8499
Epoch 1/1... Batch 4370... Discriminator Loss: 1.4077... Generator Loss: 0.7586
Epoch 1/1... Batch 4380... Discriminator Loss: 1.2816... Generator Loss: 0.7850
Epoch 1/1... Batch 4390... Discriminator Loss: 1.3159... Generator Loss: 0.8212
Epoch 1/1... Batch 4400... Discriminator Loss: 1.3342... Generator Loss: 0.8431</code></pre><p><img src="output_25_87.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 4410... Discriminator Loss: 1.3371... Generator Loss: 0.8113
Epoch 1/1... Batch 4420... Discriminator Loss: 1.3655... Generator Loss: 0.7927
Epoch 1/1... Batch 4430... Discriminator Loss: 1.2952... Generator Loss: 0.7748
Epoch 1/1... Batch 4440... Discriminator Loss: 1.3367... Generator Loss: 0.8592
Epoch 1/1... Batch 4450... Discriminator Loss: 1.3337... Generator Loss: 0.7687
Epoch 1/1... Batch 4460... Discriminator Loss: 1.2751... Generator Loss: 0.8917
Epoch 1/1... Batch 4470... Discriminator Loss: 1.3085... Generator Loss: 0.7968
Epoch 1/1... Batch 4480... Discriminator Loss: 1.2881... Generator Loss: 0.8573
Epoch 1/1... Batch 4490... Discriminator Loss: 1.4326... Generator Loss: 0.7670
Epoch 1/1... Batch 4500... Discriminator Loss: 1.3825... Generator Loss: 0.7609</code></pre><p><img src="output_25_89.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 4510... Discriminator Loss: 1.2894... Generator Loss: 0.8484
Epoch 1/1... Batch 4520... Discriminator Loss: 1.3021... Generator Loss: 0.8505
Epoch 1/1... Batch 4530... Discriminator Loss: 1.2849... Generator Loss: 0.8086
Epoch 1/1... Batch 4540... Discriminator Loss: 1.2771... Generator Loss: 0.8010
Epoch 1/1... Batch 4550... Discriminator Loss: 1.2811... Generator Loss: 0.8547
Epoch 1/1... Batch 4560... Discriminator Loss: 1.3396... Generator Loss: 0.8168
Epoch 1/1... Batch 4570... Discriminator Loss: 1.2975... Generator Loss: 0.8266
Epoch 1/1... Batch 4580... Discriminator Loss: 1.3142... Generator Loss: 0.8300
Epoch 1/1... Batch 4590... Discriminator Loss: 1.3294... Generator Loss: 0.8023
Epoch 1/1... Batch 4600... Discriminator Loss: 1.2443... Generator Loss: 0.8653</code></pre><p><img src="output_25_91.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 4610... Discriminator Loss: 1.2628... Generator Loss: 0.8326
Epoch 1/1... Batch 4620... Discriminator Loss: 1.3692... Generator Loss: 0.8792
Epoch 1/1... Batch 4630... Discriminator Loss: 1.2902... Generator Loss: 0.8560
Epoch 1/1... Batch 4640... Discriminator Loss: 1.3932... Generator Loss: 0.8111
Epoch 1/1... Batch 4650... Discriminator Loss: 1.3283... Generator Loss: 0.8021
Epoch 1/1... Batch 4660... Discriminator Loss: 1.2426... Generator Loss: 0.8108
Epoch 1/1... Batch 4670... Discriminator Loss: 1.3419... Generator Loss: 0.7889
Epoch 1/1... Batch 4680... Discriminator Loss: 1.3336... Generator Loss: 0.8802
Epoch 1/1... Batch 4690... Discriminator Loss: 1.4898... Generator Loss: 0.7315
Epoch 1/1... Batch 4700... Discriminator Loss: 1.2889... Generator Loss: 0.8196</code></pre><p><img src="output_25_93.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 4710... Discriminator Loss: 1.2783... Generator Loss: 0.8716
Epoch 1/1... Batch 4720... Discriminator Loss: 1.2672... Generator Loss: 0.8013
Epoch 1/1... Batch 4730... Discriminator Loss: 1.2786... Generator Loss: 0.8099
Epoch 1/1... Batch 4740... Discriminator Loss: 1.3510... Generator Loss: 0.8710
Epoch 1/1... Batch 4750... Discriminator Loss: 1.2683... Generator Loss: 0.8063
Epoch 1/1... Batch 4760... Discriminator Loss: 1.3469... Generator Loss: 0.7749
Epoch 1/1... Batch 4770... Discriminator Loss: 1.3882... Generator Loss: 0.7525
Epoch 1/1... Batch 4780... Discriminator Loss: 1.2547... Generator Loss: 0.8079
Epoch 1/1... Batch 4790... Discriminator Loss: 1.2679... Generator Loss: 0.7640
Epoch 1/1... Batch 4800... Discriminator Loss: 1.3050... Generator Loss: 0.7875</code></pre><p><img src="output_25_95.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 4810... Discriminator Loss: 1.2812... Generator Loss: 0.8076
Epoch 1/1... Batch 4820... Discriminator Loss: 1.3955... Generator Loss: 0.7214
Epoch 1/1... Batch 4830... Discriminator Loss: 1.2530... Generator Loss: 0.8382
Epoch 1/1... Batch 4840... Discriminator Loss: 1.4249... Generator Loss: 0.7046
Epoch 1/1... Batch 4850... Discriminator Loss: 1.3317... Generator Loss: 0.8629
Epoch 1/1... Batch 4860... Discriminator Loss: 1.3185... Generator Loss: 0.7968
Epoch 1/1... Batch 4870... Discriminator Loss: 1.2795... Generator Loss: 0.8342
Epoch 1/1... Batch 4880... Discriminator Loss: 1.3066... Generator Loss: 0.7702
Epoch 1/1... Batch 4890... Discriminator Loss: 1.3028... Generator Loss: 0.8555
Epoch 1/1... Batch 4900... Discriminator Loss: 1.4136... Generator Loss: 0.8053</code></pre><p><img src="output_25_97.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 4910... Discriminator Loss: 1.3422... Generator Loss: 0.8273
Epoch 1/1... Batch 4920... Discriminator Loss: 1.3285... Generator Loss: 0.8174
Epoch 1/1... Batch 4930... Discriminator Loss: 1.3329... Generator Loss: 0.8654
Epoch 1/1... Batch 4940... Discriminator Loss: 1.2712... Generator Loss: 0.8232
Epoch 1/1... Batch 4950... Discriminator Loss: 1.3622... Generator Loss: 0.8284
Epoch 1/1... Batch 4960... Discriminator Loss: 1.3010... Generator Loss: 0.8655
Epoch 1/1... Batch 4970... Discriminator Loss: 1.3434... Generator Loss: 0.8183
Epoch 1/1... Batch 4980... Discriminator Loss: 1.3286... Generator Loss: 0.8394
Epoch 1/1... Batch 4990... Discriminator Loss: 1.3694... Generator Loss: 0.7297
Epoch 1/1... Batch 5000... Discriminator Loss: 1.3579... Generator Loss: 0.7389</code></pre><p><img src="output_25_99.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 5010... Discriminator Loss: 1.2931... Generator Loss: 0.8143
Epoch 1/1... Batch 5020... Discriminator Loss: 1.3720... Generator Loss: 0.7953
Epoch 1/1... Batch 5030... Discriminator Loss: 1.3592... Generator Loss: 0.7495
Epoch 1/1... Batch 5040... Discriminator Loss: 1.3033... Generator Loss: 0.8248
Epoch 1/1... Batch 5050... Discriminator Loss: 1.3155... Generator Loss: 0.7942
Epoch 1/1... Batch 5060... Discriminator Loss: 1.4277... Generator Loss: 0.7672
Epoch 1/1... Batch 5070... Discriminator Loss: 1.2471... Generator Loss: 0.8519
Epoch 1/1... Batch 5080... Discriminator Loss: 1.2884... Generator Loss: 0.8086
Epoch 1/1... Batch 5090... Discriminator Loss: 1.4163... Generator Loss: 0.8031
Epoch 1/1... Batch 5100... Discriminator Loss: 1.3056... Generator Loss: 0.7579</code></pre><p><img src="output_25_101.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 5110... Discriminator Loss: 1.4004... Generator Loss: 0.7861
Epoch 1/1... Batch 5120... Discriminator Loss: 1.3525... Generator Loss: 0.7271
Epoch 1/1... Batch 5130... Discriminator Loss: 1.3151... Generator Loss: 0.8591
Epoch 1/1... Batch 5140... Discriminator Loss: 1.3548... Generator Loss: 0.8789
Epoch 1/1... Batch 5150... Discriminator Loss: 1.2994... Generator Loss: 0.8313
Epoch 1/1... Batch 5160... Discriminator Loss: 1.3001... Generator Loss: 0.8471
Epoch 1/1... Batch 5170... Discriminator Loss: 1.3916... Generator Loss: 0.7823
Epoch 1/1... Batch 5180... Discriminator Loss: 1.2965... Generator Loss: 0.8101
Epoch 1/1... Batch 5190... Discriminator Loss: 1.3385... Generator Loss: 0.7720
Epoch 1/1... Batch 5200... Discriminator Loss: 1.2931... Generator Loss: 0.7996</code></pre><p><img src="output_25_103.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 5210... Discriminator Loss: 1.3912... Generator Loss: 0.7650
Epoch 1/1... Batch 5220... Discriminator Loss: 1.3143... Generator Loss: 0.8114
Epoch 1/1... Batch 5230... Discriminator Loss: 1.3636... Generator Loss: 0.8309
Epoch 1/1... Batch 5240... Discriminator Loss: 1.3117... Generator Loss: 0.7960
Epoch 1/1... Batch 5250... Discriminator Loss: 1.3172... Generator Loss: 0.8896
Epoch 1/1... Batch 5260... Discriminator Loss: 1.2934... Generator Loss: 0.7812
Epoch 1/1... Batch 5270... Discriminator Loss: 1.3503... Generator Loss: 0.8113
Epoch 1/1... Batch 5280... Discriminator Loss: 1.3331... Generator Loss: 0.8388
Epoch 1/1... Batch 5290... Discriminator Loss: 1.3293... Generator Loss: 0.7792
Epoch 1/1... Batch 5300... Discriminator Loss: 1.3105... Generator Loss: 0.8215</code></pre><p><img src="output_25_105.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 5310... Discriminator Loss: 1.3783... Generator Loss: 0.8099
Epoch 1/1... Batch 5320... Discriminator Loss: 1.3120... Generator Loss: 0.7954
Epoch 1/1... Batch 5330... Discriminator Loss: 1.3131... Generator Loss: 0.8201
Epoch 1/1... Batch 5340... Discriminator Loss: 1.3457... Generator Loss: 0.8468
Epoch 1/1... Batch 5350... Discriminator Loss: 1.3018... Generator Loss: 0.8956
Epoch 1/1... Batch 5360... Discriminator Loss: 1.3404... Generator Loss: 0.8397
Epoch 1/1... Batch 5370... Discriminator Loss: 1.3293... Generator Loss: 0.7834
Epoch 1/1... Batch 5380... Discriminator Loss: 1.3534... Generator Loss: 0.7847
Epoch 1/1... Batch 5390... Discriminator Loss: 1.2718... Generator Loss: 0.8220
Epoch 1/1... Batch 5400... Discriminator Loss: 1.3758... Generator Loss: 0.8340</code></pre><p><img src="output_25_107.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 5410... Discriminator Loss: 1.3026... Generator Loss: 0.8132
Epoch 1/1... Batch 5420... Discriminator Loss: 1.2829... Generator Loss: 0.8684
Epoch 1/1... Batch 5430... Discriminator Loss: 1.3299... Generator Loss: 0.7528
Epoch 1/1... Batch 5440... Discriminator Loss: 1.3757... Generator Loss: 0.7133
Epoch 1/1... Batch 5450... Discriminator Loss: 1.3801... Generator Loss: 0.7795
Epoch 1/1... Batch 5460... Discriminator Loss: 1.3484... Generator Loss: 0.8204
Epoch 1/1... Batch 5470... Discriminator Loss: 1.2706... Generator Loss: 0.7948
Epoch 1/1... Batch 5480... Discriminator Loss: 1.3762... Generator Loss: 0.8431
Epoch 1/1... Batch 5490... Discriminator Loss: 1.3252... Generator Loss: 0.8304
Epoch 1/1... Batch 5500... Discriminator Loss: 1.3369... Generator Loss: 0.7729</code></pre><p><img src="output_25_109.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 5510... Discriminator Loss: 1.4130... Generator Loss: 0.7850
Epoch 1/1... Batch 5520... Discriminator Loss: 1.3532... Generator Loss: 0.7734
Epoch 1/1... Batch 5530... Discriminator Loss: 1.3917... Generator Loss: 0.8346
Epoch 1/1... Batch 5540... Discriminator Loss: 1.3131... Generator Loss: 0.8338
Epoch 1/1... Batch 5550... Discriminator Loss: 1.3492... Generator Loss: 0.7909
Epoch 1/1... Batch 5560... Discriminator Loss: 1.3527... Generator Loss: 0.7725
Epoch 1/1... Batch 5570... Discriminator Loss: 1.2277... Generator Loss: 0.9202
Epoch 1/1... Batch 5580... Discriminator Loss: 1.3408... Generator Loss: 0.7758
Epoch 1/1... Batch 5590... Discriminator Loss: 1.3723... Generator Loss: 0.8284
Epoch 1/1... Batch 5600... Discriminator Loss: 1.3419... Generator Loss: 0.8556</code></pre><p><img src="output_25_111.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 5610... Discriminator Loss: 1.3395... Generator Loss: 0.8150
Epoch 1/1... Batch 5620... Discriminator Loss: 1.3994... Generator Loss: 0.7947
Epoch 1/1... Batch 5630... Discriminator Loss: 1.3185... Generator Loss: 0.7919
Epoch 1/1... Batch 5640... Discriminator Loss: 1.2763... Generator Loss: 0.7977
Epoch 1/1... Batch 5650... Discriminator Loss: 1.2532... Generator Loss: 0.8619
Epoch 1/1... Batch 5660... Discriminator Loss: 1.3280... Generator Loss: 0.8334
Epoch 1/1... Batch 5670... Discriminator Loss: 1.3137... Generator Loss: 0.8216
Epoch 1/1... Batch 5680... Discriminator Loss: 1.3308... Generator Loss: 0.7581
Epoch 1/1... Batch 5690... Discriminator Loss: 1.3349... Generator Loss: 0.8375
Epoch 1/1... Batch 5700... Discriminator Loss: 1.2810... Generator Loss: 0.8281</code></pre><p><img src="output_25_113.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 5710... Discriminator Loss: 1.3527... Generator Loss: 0.8045
Epoch 1/1... Batch 5720... Discriminator Loss: 1.3274... Generator Loss: 0.7892
Epoch 1/1... Batch 5730... Discriminator Loss: 1.3928... Generator Loss: 0.7633
Epoch 1/1... Batch 5740... Discriminator Loss: 1.3577... Generator Loss: 0.7578
Epoch 1/1... Batch 5750... Discriminator Loss: 1.3330... Generator Loss: 0.7693
Epoch 1/1... Batch 5760... Discriminator Loss: 1.3158... Generator Loss: 0.8733
Epoch 1/1... Batch 5770... Discriminator Loss: 1.3807... Generator Loss: 0.7759
Epoch 1/1... Batch 5780... Discriminator Loss: 1.3689... Generator Loss: 0.8037
Epoch 1/1... Batch 5790... Discriminator Loss: 1.3303... Generator Loss: 0.7940
Epoch 1/1... Batch 5800... Discriminator Loss: 1.3786... Generator Loss: 0.8264</code></pre><p><img src="output_25_115.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 5810... Discriminator Loss: 1.4830... Generator Loss: 0.7643
Epoch 1/1... Batch 5820... Discriminator Loss: 1.2637... Generator Loss: 0.8704
Epoch 1/1... Batch 5830... Discriminator Loss: 1.4025... Generator Loss: 0.7605
Epoch 1/1... Batch 5840... Discriminator Loss: 1.3078... Generator Loss: 0.8410
Epoch 1/1... Batch 5850... Discriminator Loss: 1.3142... Generator Loss: 0.7959
Epoch 1/1... Batch 5860... Discriminator Loss: 1.3680... Generator Loss: 0.8228
Epoch 1/1... Batch 5870... Discriminator Loss: 1.3091... Generator Loss: 0.7886
Epoch 1/1... Batch 5880... Discriminator Loss: 1.3858... Generator Loss: 0.7332
Epoch 1/1... Batch 5890... Discriminator Loss: 1.4162... Generator Loss: 0.7355
Epoch 1/1... Batch 5900... Discriminator Loss: 1.3403... Generator Loss: 0.7873</code></pre><p><img src="output_25_117.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 5910... Discriminator Loss: 1.3423... Generator Loss: 0.8189
Epoch 1/1... Batch 5920... Discriminator Loss: 1.3084... Generator Loss: 0.8688
Epoch 1/1... Batch 5930... Discriminator Loss: 1.3679... Generator Loss: 0.7967
Epoch 1/1... Batch 5940... Discriminator Loss: 1.3655... Generator Loss: 0.7412
Epoch 1/1... Batch 5950... Discriminator Loss: 1.3869... Generator Loss: 0.7958
Epoch 1/1... Batch 5960... Discriminator Loss: 1.2983... Generator Loss: 0.8229
Epoch 1/1... Batch 5970... Discriminator Loss: 1.3505... Generator Loss: 0.7503
Epoch 1/1... Batch 5980... Discriminator Loss: 1.4133... Generator Loss: 0.7562
Epoch 1/1... Batch 5990... Discriminator Loss: 1.3111... Generator Loss: 0.8317
Epoch 1/1... Batch 6000... Discriminator Loss: 1.3121... Generator Loss: 0.7656</code></pre><p><img src="output_25_119.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 6010... Discriminator Loss: 1.3509... Generator Loss: 0.8216
Epoch 1/1... Batch 6020... Discriminator Loss: 1.3577... Generator Loss: 0.8425
Epoch 1/1... Batch 6030... Discriminator Loss: 1.3097... Generator Loss: 0.8263
Epoch 1/1... Batch 6040... Discriminator Loss: 1.3628... Generator Loss: 0.8213
Epoch 1/1... Batch 6050... Discriminator Loss: 1.4178... Generator Loss: 0.7798
Epoch 1/1... Batch 6060... Discriminator Loss: 1.3381... Generator Loss: 0.7714
Epoch 1/1... Batch 6070... Discriminator Loss: 1.3002... Generator Loss: 0.9426
Epoch 1/1... Batch 6080... Discriminator Loss: 1.4101... Generator Loss: 0.7940
Epoch 1/1... Batch 6090... Discriminator Loss: 1.3257... Generator Loss: 0.7764
Epoch 1/1... Batch 6100... Discriminator Loss: 1.2809... Generator Loss: 0.8637</code></pre><p><img src="output_25_121.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 6110... Discriminator Loss: 1.4133... Generator Loss: 0.8441
Epoch 1/1... Batch 6120... Discriminator Loss: 1.3348... Generator Loss: 0.8041
Epoch 1/1... Batch 6130... Discriminator Loss: 1.3513... Generator Loss: 0.7641
Epoch 1/1... Batch 6140... Discriminator Loss: 1.3314... Generator Loss: 0.8086
Epoch 1/1... Batch 6150... Discriminator Loss: 1.3151... Generator Loss: 0.8230
Epoch 1/1... Batch 6160... Discriminator Loss: 1.3487... Generator Loss: 0.8011
Epoch 1/1... Batch 6170... Discriminator Loss: 1.3268... Generator Loss: 0.8102
Epoch 1/1... Batch 6180... Discriminator Loss: 1.3856... Generator Loss: 0.7406
Epoch 1/1... Batch 6190... Discriminator Loss: 1.2937... Generator Loss: 0.8339
Epoch 1/1... Batch 6200... Discriminator Loss: 1.3867... Generator Loss: 0.7794</code></pre><p><img src="output_25_123.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 6210... Discriminator Loss: 1.3403... Generator Loss: 0.8104
Epoch 1/1... Batch 6220... Discriminator Loss: 1.3504... Generator Loss: 0.8417
Epoch 1/1... Batch 6230... Discriminator Loss: 1.3556... Generator Loss: 0.8188
Epoch 1/1... Batch 6240... Discriminator Loss: 1.3525... Generator Loss: 0.8092
Epoch 1/1... Batch 6250... Discriminator Loss: 1.2981... Generator Loss: 0.8710
Epoch 1/1... Batch 6260... Discriminator Loss: 1.3105... Generator Loss: 0.7984
Epoch 1/1... Batch 6270... Discriminator Loss: 1.3825... Generator Loss: 0.8008
Epoch 1/1... Batch 6280... Discriminator Loss: 1.2986... Generator Loss: 0.7929
Epoch 1/1... Batch 6290... Discriminator Loss: 1.3711... Generator Loss: 0.7805
Epoch 1/1... Batch 6300... Discriminator Loss: 1.3232... Generator Loss: 0.8297</code></pre><p><img src="output_25_125.png" alt="png"></p>
<pre><code>Epoch 1/1... Batch 6310... Discriminator Loss: 1.3037... Generator Loss: 0.8260
Epoch 1/1... Batch 6320... Discriminator Loss: 1.3527... Generator Loss: 0.7256
Epoch 1/1... Batch 6330... Discriminator Loss: 1.3105... Generator Loss: 0.8234</code></pre><h3 id="提交项目"><a href="#提交项目" class="headerlink" title="提交项目"></a>提交项目</h3><p>提交本项目前，确保运行所有 cells 后保存该文件。</p>
<p>保存该文件为 “dlnd_face_generation.ipynb”， 并另存为 HTML 格式 “File” -&gt; “Download as”。提交项目时请附带 “helper.py” 和 “problem_unittests.py” 文件。</p>

      
    </div>

    

    
    
    

    <div>
      
        
<div class="my_post_copyright">
  <script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>
  <!-- JS库 sweetalert 可修改路径 -->
  <script type="text/javascript" src="http://jslibs.wuxubj.cn/sweetalert_mini/jquery-1.7.1.min.js"></script>
  <script src="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.min.js"></script>
  <link rel="stylesheet" type="text/css" href="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.mini.css">
  <p><span>本文标题:</span><a href="/2019/06/27/1/">Face Generation</a></p>
  <p><span>文章作者:</span><a href="/" title="访问 Yinmin Zhang 的个人博客">Yinmin Zhang</a></p>
  <p><span>发布时间:</span>2019年06月27日 - 14:06</p>
  <p><span>最后更新:</span>2019年06月26日 - 21:06</p>
  <p><span>原始链接:</span><a href="/2019/06/27/1/" title="Face Generation">https://yinminzhang.github.io/blog/2019/06/27/1/</a>
    <span class="copy-path" title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="https://yinminzhang.github.io/blog/2019/06/27/1/" aria-label="复制成功！"></i></span>
  </p>
  <p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p>  
</div>
<script> 
    var clipboard = new Clipboard('.fa-clipboard');
    clipboard.on('success', $(function(){
      $(".fa-clipboard").click(function(){
        swal({   
          title: "",   
          text: '复制成功',   
          html: false,
          timer: 500,   
          showConfirmButton: false
        });
      });
    }));  
</script>


      
    </div>

    

    
      
    
    

    

    <footer class="post-footer">
      
        
          
        
        <div class="post-tags">
          
            <a href="/tags/python/" rel="tag"><i class="fa fa-tag"></i> python</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/06/26/1/" rel="next" title="Dog classification">
                <i class="fa fa-chevron-left"></i> Dog classification
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/06/28/1/" rel="prev" title="Quadcopter Project">
                Quadcopter Project <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Yinmin Zhang">
            
              <p class="site-author-name" itemprop="name">Yinmin Zhang</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          <!--my custom code begin-->
          <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.0/jquery.min.js"></script>
          <script src="https://cdnjs.cloudflare.com/ajax/libs/velocity/1.5.0/velocity.min.js"></script>
          <script type="text/javascript">
            $("#sidebar").hover(function(){
              $("#mydivshow").velocity('stop').velocity({opacity: 1});
            },function(){
              $("#mydivshow").velocity('stop').velocity({opacity: 0});
            });
          </script>
          <div id="mydivshow" class="mydivshow">
          <!--my custom code end-->

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">7</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">6</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">4</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/yinminzhang" title="GitHub &rarr; https://github.com/yinminzhang" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:YisraelZhang@gmail.com" title="E-Mail &rarr; mailto:YisraelZhang@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://instagram.com/yourname" title="QQ &rarr; https://instagram.com/yourname" rel="noopener" target="_blank"><i class="fa fa-fw fa-qq"></i>QQ</a>
                </span>
              
            </div>
          

          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://ruoyugao.github.io/" title="https://ruoyugao.github.io/" rel="noopener" target="_blank">Aron.Gao</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="300" height="100" src="//music.163.com/outchain/player?type=1&id=74267010&auto=0&height=80"></iframe>

        </div>
      </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#人脸生成（Face-Generation）"><span class="nav-number">1.</span> <span class="nav-text">人脸生成（Face Generation）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#获取数据"><span class="nav-number">1.0.1.</span> <span class="nav-text">获取数据</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#探索数据（Explore-the-Data）"><span class="nav-number">1.1.</span> <span class="nav-text">探索数据（Explore the Data）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#MNIST"><span class="nav-number">1.1.1.</span> <span class="nav-text">MNIST</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CelebA"><span class="nav-number">1.1.2.</span> <span class="nav-text">CelebA</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#预处理数据（Preprocess-the-Data）"><span class="nav-number">1.2.</span> <span class="nav-text">预处理数据（Preprocess the Data）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#建立神经网络（Build-the-Neural-Network）"><span class="nav-number">1.3.</span> <span class="nav-text">建立神经网络（Build the Neural Network）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#检查-TensorFlow-版本并获取-GPU-型号"><span class="nav-number">1.3.1.</span> <span class="nav-text">检查 TensorFlow 版本并获取 GPU 型号</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#输入（Input）"><span class="nav-number">1.3.2.</span> <span class="nav-text">输入（Input）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#辨别器（Discriminator）"><span class="nav-number">1.3.3.</span> <span class="nav-text">辨别器（Discriminator）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#生成器（Generator）"><span class="nav-number">1.3.4.</span> <span class="nav-text">生成器（Generator）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#损失函数（Loss）"><span class="nav-number">1.3.5.</span> <span class="nav-text">损失函数（Loss）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#优化（Optimization）"><span class="nav-number">1.3.6.</span> <span class="nav-text">优化（Optimization）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#训练神经网络（Neural-Network-Training）"><span class="nav-number">1.4.</span> <span class="nav-text">训练神经网络（Neural Network Training）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#输出显示"><span class="nav-number">1.4.1.</span> <span class="nav-text">输出显示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#训练"><span class="nav-number">1.4.2.</span> <span class="nav-text">训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MNIST-1"><span class="nav-number">1.4.3.</span> <span class="nav-text">MNIST</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CelebA-1"><span class="nav-number">1.4.4.</span> <span class="nav-text">CelebA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#提交项目"><span class="nav-number">1.4.5.</span> <span class="nav-text">提交项目</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yinmin Zhang</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">站点总字数：</span>
    
    <span title="站点总字数">197k</span>
  

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    
    <span title="站点阅读时长">2:59</span>
  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.1.2</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>














  
    
    
  
  <script color="0,104,183" opacity="1" zindex="-2" count="120" src="/lib/canvas-nest/canvas-nest.min.js"></script>













  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.1.2"></script>

  <script src="/js/motion.js?v=7.1.2"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.1.2"></script>




  
  <script src="/js/scrollspy.js?v=7.1.2"></script>
<script src="/js/post-details.js?v=7.1.2"></script>



  


  <script src="/js/next-boot.js?v=7.1.2"></script>


  

  

  

  
  

<script src="//unpkg.com/valine/dist/Valine.min.js"></script>

<script>
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: true,
    notify: true,
    appId: 'SDKAuIGHTmlV9h54JYlTEOra-MdYXbMMI',
    appKey: 'TeSbB5xl4sdjOLoj4TsMNn0g',
    placeholder: '麻衣学姐世界第一可爱!',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false,
    lang: '' || 'zh-cn'
  });
</script>




  


  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  
  
  <script>
    
    function addCount(Counter) {
      var $visitors = $('.leancloud_visitors');
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();

      Counter('get', '/classes/Counter', { where: JSON.stringify({ url }) })
        .done(function({ results }) {
          if (results.length > 0) {
            var counter = results[0];
            
            Counter('put', '/classes/Counter/' + counter.objectId, JSON.stringify({ time: { '__op': 'Increment', 'amount': 1 } }))
            
              .done(function() {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.time + 1);
              })
            
              .fail(function ({ responseJSON }) {
                console.log('Failed to save Visitor num, with error message: ' + responseJSON.error);
              })
          } else {
            
              var $element = $(document.getElementById(url));
              $element.find('.leancloud-visitors-count').text('Counter not initialized! More info at console err msg.');
              console.error('ATTENTION! LeanCloud counter has security bug, see how to solve it here: https://github.com/theme-next/hexo-leancloud-counter-security. \n However, you can still use LeanCloud without security, by setting `security` option to `false`.');
            
          }
        })
        .fail(function ({ responseJSON }) {
          console.log('LeanCloud Counter Error: ' + responseJSON.code + ' ' + responseJSON.error);
        });
    }
    

    $(function() {
      $.get('https://app-router.leancloud.cn/2/route?appId=' + 'SDKAuIGHTmlV9h54JYlTEOra-MdYXbMMI')
        .done(function({ api_server }) {
          var Counter = function(method, url, data) {
            return $.ajax({
              method: method,
              url: 'https://' + api_server + '/1.1' + url,
              headers: {
                'X-LC-Id': 'SDKAuIGHTmlV9h54JYlTEOra-MdYXbMMI',
                'X-LC-Key': 'TeSbB5xl4sdjOLoj4TsMNn0g',
                'Content-Type': 'application/json',
              },
              data: data
            });
          };
          
            const localhost = /http:\/\/(localhost|127.0.0.1|0.0.0.0)/;
            if (localhost.test(document.URL)) return;
            addCount(Counter);
          
        });
    });
  </script>



  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
